{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting Heart Disease\n",
        "\n",
        "## Score: .95374"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict, KFold\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "import catboost as cb\n",
        "from catboost import CatBoostClassifier\n",
        "#%pip install torch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DATA_DIR = Path(\"playground-series-s6e2\")\n",
        "OUTPUT_DIR = Path(\".\")\n",
        "n_splits = 5\n",
        "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (630000, 15)\n",
            "Test: (270000, 14)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Chest pain type</th>\n",
              "      <th>BP</th>\n",
              "      <th>Cholesterol</th>\n",
              "      <th>FBS over 120</th>\n",
              "      <th>EKG results</th>\n",
              "      <th>Max HR</th>\n",
              "      <th>Exercise angina</th>\n",
              "      <th>ST depression</th>\n",
              "      <th>Slope of ST</th>\n",
              "      <th>Number of vessels fluro</th>\n",
              "      <th>Thallium</th>\n",
              "      <th>Heart Disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>152</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>158</td>\n",
              "      <td>1</td>\n",
              "      <td>3.6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Presence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>325</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>160</td>\n",
              "      <td>188</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>134</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>140</td>\n",
              "      <td>234</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Presence</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n",
              "0   0   58    1                4  152          239             0            0   \n",
              "1   1   52    1                1  125          325             0            2   \n",
              "2   2   56    0                2  160          188             0            2   \n",
              "3   3   44    0                3  134          229             0            2   \n",
              "4   4   58    1                4  140          234             0            2   \n",
              "\n",
              "   Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
              "0     158                1            3.6            2   \n",
              "1     171                0            0.0            1   \n",
              "2     151                0            0.0            1   \n",
              "3     150                0            1.0            2   \n",
              "4     125                1            3.8            2   \n",
              "\n",
              "   Number of vessels fluro  Thallium Heart Disease  \n",
              "0                        2         7      Presence  \n",
              "1                        0         3       Absence  \n",
              "2                        0         3       Absence  \n",
              "3                        0         3       Absence  \n",
              "4                        3         3      Presence  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
        "test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
        "print(f\"Train: {train.shape}\")\n",
        "print(f\"Test: {test.shape}\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
            "[WinError 2] The system cannot find the file specified\n",
            "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
            "  warnings.warn(\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
            "    cpu_info = subprocess.run(\n",
            "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
            "        capture_output=True,\n",
            "        text=True,\n",
            "    )\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 554, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1039, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "                        pass_fds, cwd, env,\n",
            "                        ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "                        gid, gids, uid, umask,\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
            "                        start_new_session, process_group)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
            "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
            "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
            "                             # no special security\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<4 lines>...\n",
            "                             cwd,\n",
            "                             ^^^^\n",
            "                             startupinfo)\n",
            "                             ^^^^^^^^^^^^\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: 32 columns\n",
            "Target distribution: {0: 347546, 1: 282454}\n"
          ]
        }
      ],
      "source": [
        "target_col = \"Heart Disease\"\n",
        "id_col = \"id\"\n",
        "feature_cols = [c for c in train.columns if c not in (id_col, target_col)]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train[target_col])\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "X_test = test[feature_cols].copy()\n",
        "\n",
        "for col in feature_cols:\n",
        "    if X_train[col].isna().any() or X_test[col].isna().any():\n",
        "        med = X_train[col].median()\n",
        "        X_train[col] = X_train[col].fillna(med)\n",
        "        X_test[col] = X_test[col].fillna(med)\n",
        "\n",
        "X_train[\"chol_exercise\"] = X_train[\"Cholesterol\"] * X_train[\"Exercise angina\"]\n",
        "X_test[\"chol_exercise\"] = X_test[\"Cholesterol\"] * X_test[\"Exercise angina\"]\n",
        "X_train[\"st_slope\"] = X_train[\"ST depression\"] * X_train[\"Slope of ST\"]\n",
        "X_test[\"st_slope\"] = X_test[\"ST depression\"] * X_test[\"Slope of ST\"]\n",
        "X_train[\"hr_age\"] = X_train[\"Max HR\"] * X_train[\"Age\"]\n",
        "X_test[\"hr_age\"] = X_test[\"Max HR\"] * X_test[\"Age\"]\n",
        "X_train[\"bp_age\"] = X_train[\"BP\"] * X_train[\"Age\"]\n",
        "X_test[\"bp_age\"] = X_test[\"BP\"] * X_test[\"Age\"]\n",
        "\n",
        "te_cols = [\"Chest pain type\", \"Slope of ST\", \"Thallium\"]\n",
        "global_mean = float(y.mean())\n",
        "m = 20\n",
        "for col in te_cols:\n",
        "    agg = pd.DataFrame({\"_y\": y}).groupby(X_train[col])[\"_y\"].agg([\"mean\", \"count\"])\n",
        "    smoothed = (agg[\"count\"] * agg[\"mean\"] + m * global_mean) / (agg[\"count\"] + m)\n",
        "    X_train[col + \"_te\"] = X_train[col].map(smoothed).fillna(global_mean)\n",
        "    X_test[col + \"_te\"] = X_test[col].map(smoothed).fillna(global_mean)\n",
        "\n",
        "scaler_feat = StandardScaler()\n",
        "X_tr_s = scaler_feat.fit_transform(X_train)\n",
        "X_te_s = scaler_feat.transform(X_test)\n",
        "kmeans = KMeans(n_clusters=12, random_state=42, n_init=10)\n",
        "kmeans.fit(X_tr_s)\n",
        "for i in range(kmeans.n_clusters):\n",
        "    d_tr = np.linalg.norm(X_tr_s - kmeans.cluster_centers_[i], axis=1)\n",
        "    d_te = np.linalg.norm(X_te_s - kmeans.cluster_centers_[i], axis=1)\n",
        "    X_train[f\"dist_c{i}\"] = d_tr\n",
        "    X_test[f\"dist_c{i}\"] = d_te\n",
        "\n",
        "print(f\"Features: {len(X_train.columns)} columns\")\n",
        "print(f\"Target distribution: {pd.Series(y).value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best OOF AUC: 0.95507, params: {'depth': 6, 'lr': 0.05, 'min_data_in_leaf': 15}\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\"depth\": [4, 5, 6], \"lr\": [0.03, 0.04, 0.05], \"min_data_in_leaf\": [15, 25, 35]}\n",
        "n_est = 800\n",
        "best_auc, best_params = 0, None\n",
        "for depth in param_grid[\"depth\"]:\n",
        "    for lr in param_grid[\"lr\"]:\n",
        "        for min_leaf in param_grid[\"min_data_in_leaf\"]:\n",
        "            m = cb.CatBoostClassifier(iterations=n_est, depth=depth, learning_rate=lr, min_data_in_leaf=min_leaf, subsample=0.75, colsample_bylevel=0.75, random_seed=42, verbose=0)\n",
        "            oof = cross_val_predict(m, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "            auc = roc_auc_score(y, oof)\n",
        "            if auc > best_auc:\n",
        "                best_auc, best_params = auc, {\"depth\": depth, \"lr\": lr, \"min_data_in_leaf\": min_leaf}\n",
        "print(f\"Best OOF AUC: {best_auc:.5f}, params: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV AUC: 0.95517 (blend w_xgb=0.4)\n"
          ]
        }
      ],
      "source": [
        "model = cb.CatBoostClassifier(iterations=n_est, depth=best_params[\"depth\"], learning_rate=best_params[\"lr\"], min_data_in_leaf=best_params[\"min_data_in_leaf\"], subsample=0.75, colsample_bylevel=0.75, random_seed=42, verbose=0)\n",
        "oof_cb = cross_val_predict(model, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=n_est, max_depth=5, learning_rate=0.05, min_child_weight=20, subsample=0.75, colsample_bytree=0.75, random_state=42, eval_metric=\"auc\")\n",
        "oof_xgb = cross_val_predict(xgb_model, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "best_w, best_auc = 0.0, roc_auc_score(y, oof_cb)\n",
        "for w in [0, 0.1, 0.2, 0.3, 0.4]:\n",
        "    oof_blend = (1 - w) * oof_cb + w * oof_xgb\n",
        "    auc = roc_auc_score(y, oof_blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w\n",
        "oof = (1 - best_w) * oof_cb + best_w * oof_xgb\n",
        "print(f\"CV AUC: {best_auc:.5f} (blend w_xgb={best_w})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, y)\n",
        "xgb_model.fit(X_train, y)\n",
        "test_cb = model.predict_proba(X_test)[:, 1]\n",
        "test_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "test_proba = (1 - best_w) * test_cb + best_w * test_xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher model using 13 shared columns from original dataset\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010110 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 668\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012746 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 672\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012106 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 671\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "Adversarial AUC (train vs test): 0.50147\n",
            "Sample weights (train-like vs test-like) -> min=0.350, max=2.793, mean=1.000\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012668 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 668\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009583 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 672\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012384 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 671\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 1/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90516\n",
            "[1999]\tvalidation_0-auc:0.95549\n",
            "FOLD 2/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90529\n",
            "[1999]\tvalidation_0-auc:0.95525\n",
            "FOLD 3/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90522\n",
            "[1999]\tvalidation_0-auc:0.95523\n",
            "FOLD 4/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90853\n",
            "[1999]\tvalidation_0-auc:0.95611\n",
            "FOLD 5/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90700\n",
            "[1999]\tvalidation_0-auc:0.95584\n",
            "FOLD 6/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90400\n",
            "[1999]\tvalidation_0-auc:0.95467\n",
            "FOLD 7/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90582\n",
            "[1898]\tvalidation_0-auc:0.95551\n",
            "FOLD 1/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713941\ttest: 0.6714176\tbest: 0.6714176 (0)\ttotal: 135ms\tremaining: 4m 30s\n",
            "1999:\tlearn: 0.2674413\ttest: 0.2673861\tbest: 0.2673861 (1999)\ttotal: 4m\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2673861464\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 2/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714161\ttest: 0.6713715\tbest: 0.6713715 (0)\ttotal: 132ms\tremaining: 4m 23s\n",
            "1999:\tlearn: 0.2672872\ttest: 0.2681865\tbest: 0.2681865 (1999)\ttotal: 4m 9s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2681865172\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 3/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714298\ttest: 0.6714595\tbest: 0.6714595 (0)\ttotal: 136ms\tremaining: 4m 32s\n",
            "1999:\tlearn: 0.2673467\ttest: 0.2677944\tbest: 0.2677944 (1999)\ttotal: 4m 2s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2677944322\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 4/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714626\ttest: 0.6713083\tbest: 0.6713083 (0)\ttotal: 158ms\tremaining: 5m 15s\n",
            "1999:\tlearn: 0.2678297\ttest: 0.2652511\tbest: 0.2652511 (1999)\ttotal: 4m 8s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2652510786\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 5/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714499\ttest: 0.6713306\tbest: 0.6713306 (0)\ttotal: 130ms\tremaining: 4m 19s\n",
            "1999:\tlearn: 0.2676812\ttest: 0.2661637\tbest: 0.2661637 (1999)\ttotal: 4m 6s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2661636993\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 6/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714140\ttest: 0.6714256\tbest: 0.6714256 (0)\ttotal: 140ms\tremaining: 4m 39s\n",
            "1999:\tlearn: 0.2670367\ttest: 0.2699450\tbest: 0.2699440 (1995)\ttotal: 4m 7s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2699440366\n",
            "bestIteration = 1995\n",
            "\n",
            "Shrink model to first 1996 iterations.\n",
            "FOLD 7/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714515\ttest: 0.6713104\tbest: 0.6713104 (0)\ttotal: 123ms\tremaining: 4m 6s\n",
            "1999:\tlearn: 0.2674981\ttest: 0.2668915\tbest: 0.2668915 (1999)\ttotal: 4m 5s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2668914799\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 1/7 - LGBMClassifier\n",
            "FOLD 2/7 - LGBMClassifier\n",
            "FOLD 3/7 - LGBMClassifier\n",
            "FOLD 4/7 - LGBMClassifier\n",
            "FOLD 5/7 - LGBMClassifier\n",
            "FOLD 6/7 - LGBMClassifier\n",
            "FOLD 7/7 - LGBMClassifier\n",
            "FOLD 1/7 - HistGradientBoostingClassifier\n",
            "FOLD 2/7 - HistGradientBoostingClassifier\n",
            "FOLD 3/7 - HistGradientBoostingClassifier\n",
            "FOLD 4/7 - HistGradientBoostingClassifier\n",
            "FOLD 5/7 - HistGradientBoostingClassifier\n",
            "FOLD 6/7 - HistGradientBoostingClassifier\n",
            "FOLD 7/7 - HistGradientBoostingClassifier\n",
            "\n",
            "XGBClassifier OOF AUC: 0.955444\n",
            "XGBClassifier CV AUC mean: 0.955446, std: +-0.00043\n",
            "\n",
            "CatBoostClassifier OOF AUC: 0.955558\n",
            "CatBoostClassifier CV AUC mean: 0.955560, std: +-0.00044\n",
            "\n",
            "LGBMClassifier OOF AUC: 0.954945\n",
            "LGBMClassifier CV AUC mean: 0.954949, std: +-0.00045\n",
            "\n",
            "HistGradientBoostingClassifier OOF AUC: 0.955278\n",
            "HistGradientBoostingClassifier CV AUC mean: 0.955282, std: +-0.00045\n",
            "\n",
            "Stack (LR meta, 4 models) OOF AUC: 0.955556\n",
            "FOLD 1/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90545\n",
            "[1936]\tvalidation_0-auc:0.95572\n",
            "FOLD 2/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90628\n",
            "[1776]\tvalidation_0-auc:0.95504\n",
            "FOLD 3/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90784\n",
            "[1999]\tvalidation_0-auc:0.95615\n",
            "FOLD 4/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90694\n",
            "[1875]\tvalidation_0-auc:0.95552\n",
            "FOLD 5/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90553\n",
            "[1999]\tvalidation_0-auc:0.95527\n",
            "FOLD 6/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90574\n",
            "[1999]\tvalidation_0-auc:0.95452\n",
            "FOLD 7/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90716\n",
            "[1999]\tvalidation_0-auc:0.95589\n",
            "FOLD 1/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714114\ttest: 0.6713249\tbest: 0.6713249 (0)\ttotal: 144ms\tremaining: 4m 47s\n",
            "1999:\tlearn: 0.2675757\ttest: 0.2667309\tbest: 0.2667309 (1999)\ttotal: 4m 5s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2667308975\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 2/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713719\ttest: 0.6713818\tbest: 0.6713818 (0)\ttotal: 140ms\tremaining: 4m 39s\n",
            "1999:\tlearn: 0.2672089\ttest: 0.2688151\tbest: 0.2688150 (1998)\ttotal: 4m 9s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.268815017\n",
            "bestIteration = 1998\n",
            "\n",
            "Shrink model to first 1999 iterations.\n",
            "FOLD 3/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714104\ttest: 0.6712786\tbest: 0.6712786 (0)\ttotal: 131ms\tremaining: 4m 21s\n",
            "1999:\tlearn: 0.2678293\ttest: 0.2654074\tbest: 0.2654071 (1997)\ttotal: 4m 4s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2654070676\n",
            "bestIteration = 1997\n",
            "\n",
            "Shrink model to first 1998 iterations.\n",
            "FOLD 4/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714127\ttest: 0.6713281\tbest: 0.6713281 (0)\ttotal: 138ms\tremaining: 4m 36s\n",
            "1999:\tlearn: 0.2675694\ttest: 0.2668289\tbest: 0.2668289 (1999)\ttotal: 4m 5s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2668288958\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 5/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713342\ttest: 0.6714409\tbest: 0.6714409 (0)\ttotal: 141ms\tremaining: 4m 42s\n",
            "1999:\tlearn: 0.2674022\ttest: 0.2680808\tbest: 0.2680808 (1999)\ttotal: 4m 4s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2680808349\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 6/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713900\ttest: 0.6713552\tbest: 0.6713552 (0)\ttotal: 150ms\tremaining: 5m\n",
            "1999:\tlearn: 0.2670977\ttest: 0.2701005\tbest: 0.2700999 (1996)\ttotal: 4m 5s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2700999194\n",
            "bestIteration = 1996\n",
            "\n",
            "Shrink model to first 1997 iterations.\n",
            "FOLD 7/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714381\ttest: 0.6712619\tbest: 0.6712619 (0)\ttotal: 152ms\tremaining: 5m 3s\n",
            "1999:\tlearn: 0.2676997\ttest: 0.2659555\tbest: 0.2659555 (1999)\ttotal: 4m 7s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2659554672\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 1/7 - LGBMClassifier\n",
            "FOLD 2/7 - LGBMClassifier\n",
            "FOLD 3/7 - LGBMClassifier\n",
            "FOLD 4/7 - LGBMClassifier\n",
            "FOLD 5/7 - LGBMClassifier\n",
            "FOLD 6/7 - LGBMClassifier\n",
            "FOLD 7/7 - LGBMClassifier\n",
            "FOLD 1/7 - HistGradientBoostingClassifier\n",
            "FOLD 2/7 - HistGradientBoostingClassifier\n",
            "FOLD 3/7 - HistGradientBoostingClassifier\n",
            "FOLD 4/7 - HistGradientBoostingClassifier\n",
            "FOLD 5/7 - HistGradientBoostingClassifier\n",
            "FOLD 6/7 - HistGradientBoostingClassifier\n",
            "FOLD 7/7 - HistGradientBoostingClassifier\n",
            "\n",
            "XGBClassifier OOF AUC: 0.955442\n",
            "XGBClassifier CV AUC mean: 0.955445, std: +-0.00051\n",
            "\n",
            "CatBoostClassifier OOF AUC: 0.955544\n",
            "CatBoostClassifier CV AUC mean: 0.955547, std: +-0.00052\n",
            "\n",
            "LGBMClassifier OOF AUC: 0.954949\n",
            "LGBMClassifier CV AUC mean: 0.954954, std: +-0.00052\n",
            "\n",
            "HistGradientBoostingClassifier OOF AUC: 0.955269\n",
            "HistGradientBoostingClassifier CV AUC mean: 0.955273, std: +-0.00054\n",
            "\n",
            "Stack (LR meta, 4 models) OOF AUC: 0.955545\n",
            "Submission: 4-model stack, 2-seed avg. test_proba shape: (270000,)\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import inspect\n",
        "\n",
        "SEED = 42\n",
        "NSPLITS = 7\n",
        "SEEDS = [42, 43]  # best version: 2-seed averaging\n",
        "USE_LR_STACK = False\n",
        "DROP_BP_MAX_HR = True\n",
        "\n",
        "# Reload data Kaggle-style (lowercase columns, id as index)\n",
        "path = DATA_DIR\n",
        "dfs = []\n",
        "for fl in (\"train.csv\", \"test.csv\"):\n",
        "    df = pd.read_csv(path / fl, index_col=0)\n",
        "    df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns.tolist()]\n",
        "    dfs.append(df)\n",
        "train, test = dfs\n",
        "\n",
        "ystr = train.columns[-1]\n",
        "base_features = [c for c in train.columns if c != ystr]\n",
        "\n",
        "cols2comb = [\n",
        "    \"exercise_angina\", \"thallium\", \"chest_pain_type\",\n",
        "    \"slope_of_st\", \"sex\", \"st_depression\", \"number_of_vessels_fluro\",\n",
        "    \"ekg_results\", \"fbs_over_120\",\n",
        "]\n",
        "\n",
        "statmetrics = [\"mean\", \"count\"]\n",
        "\n",
        "X = train.drop(columns=ystr)\n",
        "y = (train[ystr] == \"Presence\").astype(int)\n",
        "\n",
        "X_test = test.copy()\n",
        "\n",
        "# Teacher model from original clinical dataset -> prior feature\n",
        "orig_path = DATA_DIR.parent / \"original-data\" / \"Heart_Disease_Prediction.csv\"\n",
        "orig_df = pd.read_csv(orig_path)\n",
        "orig_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in orig_df.columns]\n",
        "orig_ystr = orig_df.columns[-1]\n",
        "orig_X = orig_df.drop(columns=orig_ystr)\n",
        "orig_y = (orig_df[orig_ystr] == \"Presence\").astype(int)\n",
        "\n",
        "common_cols = sorted(set(orig_X.columns) & set(X.columns))\n",
        "print(f\"Teacher model using {len(common_cols)} shared columns from original dataset\")\n",
        "\n",
        "teacher = cb.CatBoostClassifier(\n",
        "    iterations=400,\n",
        "    depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.9,\n",
        "    colsample_bylevel=0.9,\n",
        "    random_seed=SEED,\n",
        "    verbose=0,\n",
        ")\n",
        "teacher.fit(orig_X[common_cols], orig_y)\n",
        "\n",
        "X[\"teacher_pred\"] = teacher.predict_proba(X[common_cols])[:, 1]\n",
        "X_test[\"teacher_pred\"] = teacher.predict_proba(X_test[common_cols])[:, 1]\n",
        "\n",
        "# Adversarial validation: train vs test\n",
        "adv_X = pd.concat([X, X_test], axis=0).reset_index(drop=True)\n",
        "adv_y = np.concatenate([\n",
        "    np.zeros(len(X), dtype=int),\n",
        "    np.ones(len(X_test), dtype=int),\n",
        "])\n",
        "adv_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "oof_adv = np.zeros(len(adv_y), dtype=float)\n",
        "for tr_adv, val_adv in adv_skf.split(adv_X, adv_y):\n",
        "    adv_clf = lgb.LGBMClassifier(\n",
        "        objective=\"binary\",\n",
        "        metric=\"auc\",\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=400,\n",
        "        num_leaves=31,\n",
        "        feature_fraction=0.9,\n",
        "        bagging_fraction=0.9,\n",
        "        bagging_freq=1,\n",
        "        min_data_in_leaf=30,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    adv_clf.fit(adv_X.iloc[tr_adv], adv_y[tr_adv])\n",
        "    oof_adv[val_adv] = adv_clf.predict_proba(adv_X.iloc[val_adv])[:, 1]\n",
        "\n",
        "auc_adv = roc_auc_score(adv_y, oof_adv)\n",
        "print(f\"Adversarial AUC (train vs test): {auc_adv:.5f}\")\n",
        "\n",
        "p_test_train = oof_adv[: len(X)]\n",
        "eps = 1e-3\n",
        "w_train = p_test_train / (1.0 - p_test_train + eps)\n",
        "w_train = w_train / w_train.mean()\n",
        "print(\n",
        "    f\"Sample weights (train-like vs test-like) -> min={w_train.min():.3f}, max={w_train.max():.3f}, mean={w_train.mean():.3f}\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_cat_feature_indices(X_):\n",
        "    return [i for i, c in enumerate(X_.columns) if c.startswith(\"CAT_\")]\n",
        "\n",
        "\n",
        "def fe_foldwise(X_tr, X_val, y_tr):\n",
        "    X_tr = X_tr.copy()\n",
        "    X_val = X_val.copy()\n",
        "\n",
        "    temp = pd.concat([X_tr, y_tr], axis=1)\n",
        "\n",
        "    # casting\n",
        "    for df in [X_tr, X_val]:\n",
        "        df[\"age>55\"] = (df[\"age\"] > 55).astype(int)\n",
        "        for col in df.columns:\n",
        "            if col == \"teacher_pred\":\n",
        "                continue\n",
        "            colname = f\"CAT_{col}\"\n",
        "            df[colname] = df[col].astype(str).astype(\"category\")\n",
        "\n",
        "    # numeric interactions and derived + bin features (from 0.954 notebook)\n",
        "    for df in [X_tr, X_val]:\n",
        "        df[\"chest_pain_type_bin\"] = (df[\"chest_pain_type\"] >= 3).astype(int)\n",
        "        df[\"st_depression_bin\"] = (df[\"st_depression\"] >= 2).astype(int)\n",
        "        df[\"number_of_vessels_fluro_bin\"] = (df[\"number_of_vessels_fluro\"] >= 2).astype(int)\n",
        "        df[\"hr_age\"] = df[\"max_hr\"] * df[\"age\"]\n",
        "        df[\"bp_age\"] = df[\"bp\"] * df[\"age\"]\n",
        "        df[\"st_slope\"] = df[\"st_depression\"] * df[\"slope_of_st\"]\n",
        "        df[\"chol_exercise\"] = df[\"cholesterol\"] * df[\"exercise_angina\"]\n",
        "        pred_max = (220 - df[\"age\"]).clip(lower=10)\n",
        "        df[\"max_hr_pct_pred\"] = df[\"max_hr\"] / pred_max\n",
        "        df[\"risk_sum\"] = df[\"number_of_vessels_fluro\"] + df[\"thallium\"] + df[\"exercise_angina\"]\n",
        "        df[\"vessels_thallium\"] = df[\"number_of_vessels_fluro\"] * df[\"thallium\"]\n",
        "\n",
        "    # target statistics + smoothed target encoding\n",
        "    global_mean = float(y_tr.mean())\n",
        "    m_smooth = 20\n",
        "    for bf in base_features:\n",
        "        stats = temp.groupby(bf)[ystr].agg(statmetrics)\n",
        "        for s in statmetrics:\n",
        "            cname = f\"target_{bf}_{s}\"\n",
        "            X_tr[cname] = X_tr[bf].map(stats[s])\n",
        "            X_val[cname] = X_val[bf].map(stats[s])\n",
        "        smoothed = (stats[\"count\"] * stats[\"mean\"] + m_smooth * global_mean) / (stats[\"count\"] + m_smooth)\n",
        "        X_tr[f\"target_{bf}_smooth\"] = X_tr[bf].map(smoothed).fillna(global_mean)\n",
        "        X_val[f\"target_{bf}_smooth\"] = X_val[bf].map(smoothed).fillna(global_mean)\n",
        "\n",
        "    # categorical combinations\n",
        "    for i, c1 in enumerate(cols2comb[:-1]):\n",
        "        for c2 in cols2comb[i + 1 :]:\n",
        "            m2 = max(X_tr[c2].max(), X_val[c2].max()) + 1\n",
        "            cname = f\"{c1}_{c2}\"\n",
        "            X_tr[cname] = (\n",
        "                (X_tr[c1] + 1 + (X_tr[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "            X_val[cname] = (\n",
        "                (X_val[c1] + 1 + (X_val[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "\n",
        "    if DROP_BP_MAX_HR:\n",
        "        X_tr = X_tr.drop(columns=[\"bp\", \"max_hr\"], errors=\"ignore\")\n",
        "        X_val = X_val.drop(columns=[\"bp\", \"max_hr\"], errors=\"ignore\")\n",
        "    return X_tr, X_val\n",
        "\n",
        "\n",
        "def fe_test(X_test_, X_train_, y_train_):\n",
        "    X_test_ = X_test_.copy()\n",
        "    temp = pd.concat([X_train_, y_train_], axis=1)\n",
        "\n",
        "    X_test_[\"age>55\"] = (X_test_[\"age\"] > 55).astype(int)\n",
        "    for col in X_test_.columns:\n",
        "        if col == \"teacher_pred\":\n",
        "            continue\n",
        "        colname = f\"CAT_{col}\"\n",
        "        X_test_[colname] = X_test_[col].astype(str).astype(\"category\")\n",
        "\n",
        "    X_test_[\"chest_pain_type_bin\"] = (X_test_[\"chest_pain_type\"] >= 3).astype(int)\n",
        "    X_test_[\"st_depression_bin\"] = (X_test_[\"st_depression\"] >= 2).astype(int)\n",
        "    X_test_[\"number_of_vessels_fluro_bin\"] = (X_test_[\"number_of_vessels_fluro\"] >= 2).astype(int)\n",
        "    X_test_[\"hr_age\"] = X_test_[\"max_hr\"] * X_test_[\"age\"]\n",
        "    X_test_[\"bp_age\"] = X_test_[\"bp\"] * X_test_[\"age\"]\n",
        "    X_test_[\"st_slope\"] = X_test_[\"st_depression\"] * X_test_[\"slope_of_st\"]\n",
        "    X_test_[\"chol_exercise\"] = X_test_[\"cholesterol\"] * X_test_[\"exercise_angina\"]\n",
        "    pred_max = (220 - X_test_[\"age\"]).clip(lower=10)\n",
        "    X_test_[\"max_hr_pct_pred\"] = X_test_[\"max_hr\"] / pred_max\n",
        "    X_test_[\"risk_sum\"] = X_test_[\"number_of_vessels_fluro\"] + X_test_[\"thallium\"] + X_test_[\"exercise_angina\"]\n",
        "    X_test_[\"vessels_thallium\"] = X_test_[\"number_of_vessels_fluro\"] * X_test_[\"thallium\"]\n",
        "\n",
        "    global_mean = float(y_train_.mean())\n",
        "    m_smooth = 20\n",
        "    for bf in base_features:\n",
        "        stats = temp.groupby(bf)[ystr].agg(statmetrics)\n",
        "        for s in statmetrics:\n",
        "            X_test_[f\"target_{bf}_{s}\"] = X_test_[bf].map(stats[s])\n",
        "        smoothed = (stats[\"count\"] * stats[\"mean\"] + m_smooth * global_mean) / (stats[\"count\"] + m_smooth)\n",
        "        X_test_[f\"target_{bf}_smooth\"] = X_test_[bf].map(smoothed).fillna(global_mean)\n",
        "\n",
        "    for i, c1 in enumerate(cols2comb[:-1]):\n",
        "        for c2 in cols2comb[i + 1 :]:\n",
        "            m2 = max(X_train_[c2].max(), X_test_[c2].max()) + 1\n",
        "            cname = f\"{c1}_{c2}\"\n",
        "            X_test_[cname] = (\n",
        "                (X_test_[c1] + 1 + (X_test_[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "\n",
        "    if DROP_BP_MAX_HR:\n",
        "        X_test_ = X_test_.drop(columns=[\"bp\", \"max_hr\"], errors=\"ignore\")\n",
        "    return X_test_\n",
        "\n",
        "\n",
        "xgboost_params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"auc\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"max_depth\": 2,\n",
        "    \"subsample\": 0.9,\n",
        "    \"colsample_bytree\": 0.9,\n",
        "    \"n_estimators\": 2000,\n",
        "    \"min_child_weight\": 10,\n",
        "    \"gamma\": 1,\n",
        "    \"reg_lambda\": 0.01,\n",
        "    \"reg_alpha\": 1.5,\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": SEED,\n",
        "    \"early_stopping_rounds\": 100,\n",
        "    \"enable_categorical\": True,\n",
        "}\n",
        "\n",
        "catboost_params = {\n",
        "    \"loss_function\": \"Logloss\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"depth\": 2,\n",
        "    \"subsample\": 0.9,\n",
        "    \"iterations\": 2000,\n",
        "    \"min_data_in_leaf\": 1,\n",
        "    \"l2_leaf_reg\": 1.002,\n",
        "    \"thread_count\": -1,\n",
        "    \"random_seed\": SEED,\n",
        "    \"early_stopping_rounds\": 100,\n",
        "    \"bootstrap_type\": \"Bernoulli\",\n",
        "}\n",
        "\n",
        "lgbm_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"n_estimators\": 1200,\n",
        "    \"num_leaves\": 20,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.9,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"min_data_in_leaf\": 30,\n",
        "    \"random_state\": SEED,\n",
        "    \"n_jobs\": -1,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "hgb_params = {\n",
        "    \"max_iter\": 2000,\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"max_depth\": 6,\n",
        "    \"early_stopping\": True,\n",
        "    \"n_iter_no_change\": 80,\n",
        "    \"validation_fraction\": 0.1,\n",
        "    \"random_state\": SEED,\n",
        "}\n",
        "models = {\n",
        "    XGBClassifier: xgboost_params,\n",
        "    CatBoostClassifier: catboost_params,\n",
        "    LGBMClassifier: lgbm_params,\n",
        "    HistGradientBoostingClassifier: hgb_params,\n",
        "}\n",
        "\n",
        "oof_list, test_proba_list = [], []\n",
        "for SEED in SEEDS:\n",
        "    teacher = cb.CatBoostClassifier(\n",
        "        iterations=400, depth=4, learning_rate=0.05, subsample=0.9,\n",
        "        colsample_bylevel=0.9, random_seed=SEED, verbose=0,\n",
        "    )\n",
        "    teacher.fit(orig_X[common_cols], orig_y)\n",
        "    X[\"teacher_pred\"] = teacher.predict_proba(X[common_cols])[:, 1]\n",
        "    X_test[\"teacher_pred\"] = teacher.predict_proba(X_test[common_cols])[:, 1]\n",
        "    adv_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "    oof_adv = np.zeros(len(adv_y), dtype=float)\n",
        "    for tr_adv, val_adv in adv_skf.split(adv_X, adv_y):\n",
        "        adv_clf = lgb.LGBMClassifier(\n",
        "            objective=\"binary\", metric=\"auc\", learning_rate=0.05, n_estimators=400,\n",
        "            num_leaves=31, feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "            min_data_in_leaf=30, random_state=SEED, n_jobs=-1,\n",
        "        )\n",
        "        adv_clf.fit(adv_X.iloc[tr_adv], adv_y[tr_adv])\n",
        "        oof_adv[val_adv] = adv_clf.predict_proba(adv_X.iloc[val_adv])[:, 1]\n",
        "    p_test_train = oof_adv[: len(X)]\n",
        "    w_train = p_test_train / (1.0 - p_test_train + 1e-3)\n",
        "    w_train = w_train / w_train.mean()\n",
        "    xgboost_params = {**xgboost_params, \"random_state\": SEED}\n",
        "    catboost_params = {**catboost_params, \"random_seed\": SEED}\n",
        "    lgbm_params = {**lgbm_params, \"random_state\": SEED}\n",
        "    hgb_params_s = {**hgb_params, \"random_state\": SEED}\n",
        "    models_run = {\n",
        "        XGBClassifier: xgboost_params,\n",
        "        CatBoostClassifier: catboost_params,\n",
        "        LGBMClassifier: lgbm_params,\n",
        "        HistGradientBoostingClassifier: hgb_params_s,\n",
        "    }\n",
        "    kf = KFold(n_splits=NSPLITS, shuffle=True, random_state=SEED)\n",
        "    oof_train_model = {}\n",
        "    oof_test_model = {}\n",
        "    cv_auc_model = defaultdict(list)\n",
        "    for modelClass, param in models_run.items():\n",
        "        model_name = modelClass.__name__\n",
        "        oof_train = np.zeros(len(X))\n",
        "        oof_test = np.zeros(len(X_test))\n",
        "\n",
        "        for fold, (tr, val) in enumerate(kf.split(X)):\n",
        "            print(f\"FOLD {fold + 1}/{NSPLITS} - {model_name}\")\n",
        "\n",
        "            X_tr_raw, X_val_raw = X.iloc[tr], X.iloc[val]\n",
        "            y_tr, y_val = y.iloc[tr], y.iloc[val]\n",
        "\n",
        "            X_tr, X_val = fe_foldwise(X_tr_raw, X_val_raw, y_tr)\n",
        "\n",
        "            model = modelClass(**param)\n",
        "            if model_name == \"HistGradientBoostingClassifier\":\n",
        "                X_tr_fit = X_tr.select_dtypes(include=[np.number])\n",
        "                X_val_fit = X_val.select_dtypes(include=[np.number])\n",
        "                model.fit(X_tr_fit, y_tr, sample_weight=w_train[tr])\n",
        "                oof_train[val] = model.predict_proba(X_val_fit)[:, 1]\n",
        "                X_test_fe = fe_test(X_test, X_tr_raw, y_tr)\n",
        "                X_test_fit = X_test_fe.select_dtypes(include=[np.number])\n",
        "                oof_test += model.predict_proba(X_test_fit)[:, 1] / NSPLITS\n",
        "            else:\n",
        "                fit_kwargs = {\n",
        "                    \"X\": X_tr,\n",
        "                    \"y\": y_tr,\n",
        "                    \"eval_set\": [(X_val, y_val)],\n",
        "                    \"sample_weight\": w_train[tr],\n",
        "                }\n",
        "                if model_name != \"LGBMClassifier\":\n",
        "                    fit_kwargs[\"verbose\"] = 2000\n",
        "                if \"cat_features\" in inspect.signature(model.fit).parameters:\n",
        "                    cat_features = get_cat_feature_indices(X_tr)\n",
        "                    fit_kwargs[\"cat_features\"] = cat_features\n",
        "                model.fit(**fit_kwargs)\n",
        "                oof_train[val] = model.predict_proba(X_val)[:, 1]\n",
        "                X_test_fe = fe_test(X_test, X_tr_raw, y_tr)\n",
        "                oof_test += model.predict_proba(X_test_fe)[:, 1] / NSPLITS\n",
        "            cv_auc_model[model_name].append(roc_auc_score(y[val], oof_train[val]))\n",
        "\n",
        "        oof_train_model[model_name] = oof_train\n",
        "        oof_test_model[model_name] = oof_test\n",
        "\n",
        "    # Evaluation per model (inside seed loop)\n",
        "    for modelClass in models_run.keys():\n",
        "        model_name = modelClass.__name__\n",
        "        print(f\"\\n{model_name} OOF AUC: {roc_auc_score(y, oof_train_model[model_name]):.6f}\")\n",
        "        print(\n",
        "            f\"{model_name} CV AUC mean: {np.mean(cv_auc_model[model_name]):.6f}, std: +-{np.std(cv_auc_model[model_name]):.5f}\"\n",
        "        )\n",
        "\n",
        "    # Stack: LR meta for 3+ models or when USE_LR_STACK; else tuned 2-model weight blend\n",
        "    X_oof_tr = pd.DataFrame.from_dict(oof_train_model)\n",
        "    X_oof_test = pd.DataFrame.from_dict(oof_test_model)\n",
        "    cols = list(X_oof_tr.columns)\n",
        "    use_meta = USE_LR_STACK or len(cols) >= 3\n",
        "    if use_meta:\n",
        "        meta = LogisticRegression(max_iter=500, random_state=SEED, class_weight=\"balanced\")\n",
        "        meta.fit(X_oof_tr, y, sample_weight=w_train)\n",
        "        oof_tr_final = pd.Series(meta.predict_proba(X_oof_tr)[:, 1], index=X_oof_tr.index)\n",
        "        oof_test_final = pd.Series(meta.predict_proba(X_oof_test)[:, 1], index=X_oof_test.index)\n",
        "        stack_auc = roc_auc_score(y, oof_tr_final)\n",
        "        print(f\"\\nStack (LR meta, {len(cols)} models) OOF AUC: {stack_auc:.6f}\")\n",
        "        oof_list.append(oof_tr_final.values)\n",
        "        test_proba_list.append(oof_test_final.values)\n",
        "    else:\n",
        "        a, b = X_oof_tr[cols[0]], X_oof_tr[cols[1]]\n",
        "        best_w, best_auc = 0.5, 0.0\n",
        "        for w in np.linspace(0, 1, 21):\n",
        "            blend = w * a + (1 - w) * b\n",
        "            auc = roc_auc_score(y, blend)\n",
        "            if auc > best_auc:\n",
        "                best_auc, best_w = auc, w\n",
        "        oof_tr_final = best_w * X_oof_tr[cols[0]] + (1 - best_w) * X_oof_tr[cols[1]]\n",
        "        oof_test_final = best_w * X_oof_test[cols[0]] + (1 - best_w) * X_oof_test[cols[1]]\n",
        "        print(f\"\\nBlend weight {cols[0]}={best_w:.2f}, {cols[1]}={1-best_w:.2f} -> OOF AUC: {best_auc:.6f}\")\n",
        "        oof_list.append(oof_tr_final.values)\n",
        "        test_proba_list.append(oof_test_final.values)\n",
        "\n",
        "# Seed averaging (outside SEED loop)\n",
        "oof = np.mean(oof_list, axis=0)\n",
        "test_proba = np.mean(test_proba_list, axis=0)\n",
        "N_STACK_MODELS = len(oof_train_model)\n",
        "print(f\"Submission: {N_STACK_MODELS}-model stack, {len(SEEDS)}-seed avg. test_proba shape: {test_proba.shape}\")\n",
        "\n",
        "# Ensure id column exists for submission\n",
        "test[\"id\"] = test.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix (OOF, threshold=0.5)\n",
            "Rows: true, Cols: predicted |  Absence   Presence\n",
            "[[311850  35696]\n",
            " [ 34308 248146]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "y_pred = (oof >= 0.5).astype(int)\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "print(\"Confusion matrix (OOF, threshold=0.5)\")\n",
        "print(\"Rows: true, Cols: predicted |  Absence   Presence\")\n",
        "print(cm)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(cm, cmap=\"Blues\")\n",
        "plt.colorbar()\n",
        "plt.xticks([0, 1], [\"Absence\", \"Presence\"])\n",
        "plt.yticks([0, 1], [\"Absence\", \"Presence\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
        "plt.title(\"Confusion matrix (OOF)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission saved (4-model blend): submission.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Heart Disease</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>630000</th>\n",
              "      <td>630000</td>\n",
              "      <td>0.958230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630001</th>\n",
              "      <td>630001</td>\n",
              "      <td>0.048153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630002</th>\n",
              "      <td>630002</td>\n",
              "      <td>0.967206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630003</th>\n",
              "      <td>630003</td>\n",
              "      <td>0.046752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630004</th>\n",
              "      <td>630004</td>\n",
              "      <td>0.167473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630005</th>\n",
              "      <td>630005</td>\n",
              "      <td>0.966365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630006</th>\n",
              "      <td>630006</td>\n",
              "      <td>0.045968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630007</th>\n",
              "      <td>630007</td>\n",
              "      <td>0.627969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630008</th>\n",
              "      <td>630008</td>\n",
              "      <td>0.967709</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630009</th>\n",
              "      <td>630009</td>\n",
              "      <td>0.049288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  Heart Disease\n",
              "id                           \n",
              "630000  630000       0.958230\n",
              "630001  630001       0.048153\n",
              "630002  630002       0.967206\n",
              "630003  630003       0.046752\n",
              "630004  630004       0.167473\n",
              "630005  630005       0.966365\n",
              "630006  630006       0.045968\n",
              "630007  630007       0.627969\n",
              "630008  630008       0.967709\n",
              "630009  630009       0.049288"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub = pd.DataFrame({\"id\": test[\"id\"], \"Heart Disease\": test_proba})\n",
        "sub.to_csv(OUTPUT_DIR / \"submission.csv\", index=False)\n",
        "print(f\"Submission saved ({N_STACK_MODELS}-model blend): {OUTPUT_DIR / 'submission.csv'}\")\n",
        "sub.head(10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
