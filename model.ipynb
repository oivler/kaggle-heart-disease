{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting Heart Disease\n",
        "\n",
        "## Score: .95377"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict, KFold\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "import catboost as cb\n",
        "from catboost import CatBoostClassifier\n",
        "#%pip install torch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DATA_DIR = Path(\"playground-series-s6e2\")\n",
        "OUTPUT_DIR = Path(\".\")\n",
        "n_splits = 5\n",
        "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (630000, 15)\n",
            "Test: (270000, 14)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Chest pain type</th>\n",
              "      <th>BP</th>\n",
              "      <th>Cholesterol</th>\n",
              "      <th>FBS over 120</th>\n",
              "      <th>EKG results</th>\n",
              "      <th>Max HR</th>\n",
              "      <th>Exercise angina</th>\n",
              "      <th>ST depression</th>\n",
              "      <th>Slope of ST</th>\n",
              "      <th>Number of vessels fluro</th>\n",
              "      <th>Thallium</th>\n",
              "      <th>Heart Disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>152</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>158</td>\n",
              "      <td>1</td>\n",
              "      <td>3.6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Presence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>325</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>160</td>\n",
              "      <td>188</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>134</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>140</td>\n",
              "      <td>234</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Presence</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n",
              "0   0   58    1                4  152          239             0            0   \n",
              "1   1   52    1                1  125          325             0            2   \n",
              "2   2   56    0                2  160          188             0            2   \n",
              "3   3   44    0                3  134          229             0            2   \n",
              "4   4   58    1                4  140          234             0            2   \n",
              "\n",
              "   Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
              "0     158                1            3.6            2   \n",
              "1     171                0            0.0            1   \n",
              "2     151                0            0.0            1   \n",
              "3     150                0            1.0            2   \n",
              "4     125                1            3.8            2   \n",
              "\n",
              "   Number of vessels fluro  Thallium Heart Disease  \n",
              "0                        2         7      Presence  \n",
              "1                        0         3       Absence  \n",
              "2                        0         3       Absence  \n",
              "3                        0         3       Absence  \n",
              "4                        3         3      Presence  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
        "test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
        "print(f\"Train: {train.shape}\")\n",
        "print(f\"Test: {test.shape}\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
            "[WinError 2] The system cannot find the file specified\n",
            "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
            "  warnings.warn(\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
            "    cpu_info = subprocess.run(\n",
            "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
            "        capture_output=True,\n",
            "        text=True,\n",
            "    )\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 554, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1039, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "                        pass_fds, cwd, env,\n",
            "                        ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "                        gid, gids, uid, umask,\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
            "                        start_new_session, process_group)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
            "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
            "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
            "                             # no special security\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<4 lines>...\n",
            "                             cwd,\n",
            "                             ^^^^\n",
            "                             startupinfo)\n",
            "                             ^^^^^^^^^^^^\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: 32 columns\n",
            "Target distribution: {0: 347546, 1: 282454}\n"
          ]
        }
      ],
      "source": [
        "target_col = \"Heart Disease\"\n",
        "id_col = \"id\"\n",
        "feature_cols = [c for c in train.columns if c not in (id_col, target_col)]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train[target_col])\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "X_test = test[feature_cols].copy()\n",
        "\n",
        "for col in feature_cols:\n",
        "    if X_train[col].isna().any() or X_test[col].isna().any():\n",
        "        med = X_train[col].median()\n",
        "        X_train[col] = X_train[col].fillna(med)\n",
        "        X_test[col] = X_test[col].fillna(med)\n",
        "\n",
        "X_train[\"chol_exercise\"] = X_train[\"Cholesterol\"] * X_train[\"Exercise angina\"]\n",
        "X_test[\"chol_exercise\"] = X_test[\"Cholesterol\"] * X_test[\"Exercise angina\"]\n",
        "X_train[\"st_slope\"] = X_train[\"ST depression\"] * X_train[\"Slope of ST\"]\n",
        "X_test[\"st_slope\"] = X_test[\"ST depression\"] * X_test[\"Slope of ST\"]\n",
        "X_train[\"hr_age\"] = X_train[\"Max HR\"] * X_train[\"Age\"]\n",
        "X_test[\"hr_age\"] = X_test[\"Max HR\"] * X_test[\"Age\"]\n",
        "X_train[\"bp_age\"] = X_train[\"BP\"] * X_train[\"Age\"]\n",
        "X_test[\"bp_age\"] = X_test[\"BP\"] * X_test[\"Age\"]\n",
        "\n",
        "te_cols = [\"Chest pain type\", \"Slope of ST\", \"Thallium\"]\n",
        "global_mean = float(y.mean())\n",
        "m = 20\n",
        "for col in te_cols:\n",
        "    agg = pd.DataFrame({\"_y\": y}).groupby(X_train[col])[\"_y\"].agg([\"mean\", \"count\"])\n",
        "    smoothed = (agg[\"count\"] * agg[\"mean\"] + m * global_mean) / (agg[\"count\"] + m)\n",
        "    X_train[col + \"_te\"] = X_train[col].map(smoothed).fillna(global_mean)\n",
        "    X_test[col + \"_te\"] = X_test[col].map(smoothed).fillna(global_mean)\n",
        "\n",
        "scaler_feat = StandardScaler()\n",
        "X_tr_s = scaler_feat.fit_transform(X_train)\n",
        "X_te_s = scaler_feat.transform(X_test)\n",
        "kmeans = KMeans(n_clusters=12, random_state=42, n_init=10)\n",
        "kmeans.fit(X_tr_s)\n",
        "for i in range(kmeans.n_clusters):\n",
        "    d_tr = np.linalg.norm(X_tr_s - kmeans.cluster_centers_[i], axis=1)\n",
        "    d_te = np.linalg.norm(X_te_s - kmeans.cluster_centers_[i], axis=1)\n",
        "    X_train[f\"dist_c{i}\"] = d_tr\n",
        "    X_test[f\"dist_c{i}\"] = d_te\n",
        "\n",
        "print(f\"Features: {len(X_train.columns)} columns\")\n",
        "print(f\"Target distribution: {pd.Series(y).value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best OOF AUC: 0.95507, params: {'depth': 6, 'lr': 0.05, 'min_data_in_leaf': 15}\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\"depth\": [4, 5, 6], \"lr\": [0.03, 0.04, 0.05], \"min_data_in_leaf\": [15, 25, 35]}\n",
        "n_est = 800\n",
        "best_auc, best_params = 0, None\n",
        "for depth in param_grid[\"depth\"]:\n",
        "    for lr in param_grid[\"lr\"]:\n",
        "        for min_leaf in param_grid[\"min_data_in_leaf\"]:\n",
        "            m = cb.CatBoostClassifier(iterations=n_est, depth=depth, learning_rate=lr, min_data_in_leaf=min_leaf, subsample=0.75, colsample_bylevel=0.75, random_seed=42, verbose=0)\n",
        "            oof = cross_val_predict(m, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "            auc = roc_auc_score(y, oof)\n",
        "            if auc > best_auc:\n",
        "                best_auc, best_params = auc, {\"depth\": depth, \"lr\": lr, \"min_data_in_leaf\": min_leaf}\n",
        "print(f\"Best OOF AUC: {best_auc:.5f}, params: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV AUC: 0.95517 (blend w_xgb=0.4)\n"
          ]
        }
      ],
      "source": [
        "model = cb.CatBoostClassifier(iterations=n_est, depth=best_params[\"depth\"], learning_rate=best_params[\"lr\"], min_data_in_leaf=best_params[\"min_data_in_leaf\"], subsample=0.75, colsample_bylevel=0.75, random_seed=42, verbose=0)\n",
        "oof_cb = cross_val_predict(model, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=n_est, max_depth=5, learning_rate=0.05, min_child_weight=20, subsample=0.75, colsample_bytree=0.75, random_state=42, eval_metric=\"auc\")\n",
        "oof_xgb = cross_val_predict(xgb_model, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "best_w, best_auc = 0.0, roc_auc_score(y, oof_cb)\n",
        "for w in [0, 0.1, 0.2, 0.3, 0.4]:\n",
        "    oof_blend = (1 - w) * oof_cb + w * oof_xgb\n",
        "    auc = roc_auc_score(y, oof_blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w\n",
        "oof = (1 - best_w) * oof_cb + best_w * oof_xgb\n",
        "print(f\"CV AUC: {best_auc:.5f} (blend w_xgb={best_w})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, y)\n",
        "xgb_model.fit(X_train, y)\n",
        "test_cb = model.predict_proba(X_test)[:, 1]\n",
        "test_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "test_proba = (1 - best_w) * test_cb + best_w * test_xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher model using 13 shared columns from original dataset\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012243 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 668\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024361 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 672\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023834 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 671\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "Adversarial AUC (train vs test): 0.50147\n",
            "Sample weights (train-like vs test-like) -> min=0.350, max=2.793, mean=1.000\n",
            "FOLD 1/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90516\n",
            "[2000]\tvalidation_0-auc:0.95541\n",
            "[3418]\tvalidation_0-auc:0.95549\n",
            "FOLD 2/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90529\n",
            "[2000]\tvalidation_0-auc:0.95520\n",
            "[3516]\tvalidation_0-auc:0.95527\n",
            "FOLD 3/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90522\n",
            "[2000]\tvalidation_0-auc:0.95522\n",
            "[3234]\tvalidation_0-auc:0.95526\n",
            "FOLD 4/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90853\n",
            "[2000]\tvalidation_0-auc:0.95606\n",
            "[3614]\tvalidation_0-auc:0.95613\n",
            "FOLD 5/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90700\n",
            "[2000]\tvalidation_0-auc:0.95578\n",
            "[3338]\tvalidation_0-auc:0.95585\n",
            "FOLD 6/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90400\n",
            "[2000]\tvalidation_0-auc:0.95463\n",
            "[3339]\tvalidation_0-auc:0.95470\n",
            "FOLD 7/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90582\n",
            "[2000]\tvalidation_0-auc:0.95549\n",
            "[3076]\tvalidation_0-auc:0.95553\n",
            "FOLD 1/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6790053\ttest: 0.6790325\tbest: 0.6790325 (0)\ttotal: 150ms\tremaining: 12m 29s\n",
            "2000:\tlearn: 0.2680683\ttest: 0.2677917\tbest: 0.2677917 (2000)\ttotal: 4m 56s\tremaining: 7m 25s\n",
            "4000:\tlearn: 0.2670876\ttest: 0.2671352\tbest: 0.2671352 (3999)\ttotal: 9m 48s\tremaining: 2m 26s\n",
            "4999:\tlearn: 0.2668587\ttest: 0.2670511\tbest: 0.2670511 (4999)\ttotal: 12m 19s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2670510812\n",
            "bestIteration = 4999\n",
            "\n",
            "FOLD 2/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6791548\ttest: 0.6790946\tbest: 0.6790946 (0)\ttotal: 146ms\tremaining: 12m 9s\n",
            "2000:\tlearn: 0.2678558\ttest: 0.2685474\tbest: 0.2685474 (2000)\ttotal: 4m 50s\tremaining: 7m 15s\n",
            "4000:\tlearn: 0.2669437\ttest: 0.2679554\tbest: 0.2679554 (4000)\ttotal: 9m 56s\tremaining: 2m 28s\n",
            "4999:\tlearn: 0.2667211\ttest: 0.2678742\tbest: 0.2678742 (4999)\ttotal: 12m 23s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2678741855\n",
            "bestIteration = 4999\n",
            "\n",
            "FOLD 3/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6791336\ttest: 0.6791539\tbest: 0.6791539 (0)\ttotal: 145ms\tremaining: 12m 3s\n",
            "2000:\tlearn: 0.2679388\ttest: 0.2681593\tbest: 0.2681593 (2000)\ttotal: 4m 23s\tremaining: 6m 35s\n",
            "4000:\tlearn: 0.2669656\ttest: 0.2676610\tbest: 0.2676607 (3999)\ttotal: 8m 14s\tremaining: 2m 3s\n",
            "4999:\tlearn: 0.2667371\ttest: 0.2676128\tbest: 0.2676126 (4996)\ttotal: 10m 12s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2676126054\n",
            "bestIteration = 4996\n",
            "\n",
            "Shrink model to first 4997 iterations.\n",
            "FOLD 4/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6791628\ttest: 0.6790943\tbest: 0.6790943 (0)\ttotal: 125ms\tremaining: 10m 24s\n",
            "2000:\tlearn: 0.2684069\ttest: 0.2657100\tbest: 0.2657100 (2000)\ttotal: 3m 45s\tremaining: 5m 37s\n",
            "4000:\tlearn: 0.2674518\ttest: 0.2650783\tbest: 0.2650779 (3986)\ttotal: 7m 19s\tremaining: 1m 49s\n",
            "4999:\tlearn: 0.2672379\ttest: 0.2649982\tbest: 0.2649982 (4996)\ttotal: 9m 6s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2649981558\n",
            "bestIteration = 4996\n",
            "\n",
            "Shrink model to first 4997 iterations.\n",
            "FOLD 5/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6790472\ttest: 0.6789585\tbest: 0.6789585 (0)\ttotal: 130ms\tremaining: 10m 48s\n",
            "2000:\tlearn: 0.2682276\ttest: 0.2666254\tbest: 0.2666254 (2000)\ttotal: 3m 34s\tremaining: 5m 22s\n",
            "4000:\tlearn: 0.2672949\ttest: 0.2659845\tbest: 0.2659845 (4000)\ttotal: 7m 44s\tremaining: 1m 56s\n",
            "4999:\tlearn: 0.2670657\ttest: 0.2658910\tbest: 0.2658910 (4999)\ttotal: 9m 59s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.265891028\n",
            "bestIteration = 4999\n",
            "\n",
            "FOLD 6/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6790124\ttest: 0.6790385\tbest: 0.6790385 (0)\ttotal: 151ms\tremaining: 12m 36s\n",
            "2000:\tlearn: 0.2676248\ttest: 0.2703913\tbest: 0.2703897 (1998)\ttotal: 4m 19s\tremaining: 6m 29s\n",
            "4000:\tlearn: 0.2667045\ttest: 0.2697839\tbest: 0.2697839 (4000)\ttotal: 8m 50s\tremaining: 2m 12s\n",
            "4999:\tlearn: 0.2664851\ttest: 0.2696908\tbest: 0.2696897 (4973)\ttotal: 11m 5s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2696897201\n",
            "bestIteration = 4973\n",
            "\n",
            "Shrink model to first 4974 iterations.\n",
            "FOLD 7/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6791586\ttest: 0.6791148\tbest: 0.6791148 (0)\ttotal: 150ms\tremaining: 12m 28s\n",
            "2000:\tlearn: 0.2681272\ttest: 0.2672122\tbest: 0.2672120 (1999)\ttotal: 4m 32s\tremaining: 6m 48s\n",
            "4000:\tlearn: 0.2671888\ttest: 0.2666949\tbest: 0.2666949 (3999)\ttotal: 8m 51s\tremaining: 2m 12s\n",
            "4999:\tlearn: 0.2669566\ttest: 0.2666301\tbest: 0.2666299 (4988)\ttotal: 11m 1s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2666299008\n",
            "bestIteration = 4988\n",
            "\n",
            "Shrink model to first 4989 iterations.\n",
            "FOLD 1/7 - LGBMClassifier\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 242170, number of negative: 297830\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090780 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3772\n",
            "[LightGBM] [Info] Number of data points in the train set: 540000, number of used features: 113\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.450004 -> initscore=-0.200656\n",
            "[LightGBM] [Info] Start training from score -0.200656\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 2/7 - LGBMClassifier\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 242042, number of negative: 297958\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099880 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3793\n",
            "[LightGBM] [Info] Number of data points in the train set: 540000, number of used features: 113\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.449760 -> initscore=-0.201642\n",
            "[LightGBM] [Info] Start training from score -0.201642\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 3/7 - LGBMClassifier\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 241857, number of negative: 298143\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.104907 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3803\n",
            "[LightGBM] [Info] Number of data points in the train set: 540000, number of used features: 113\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.449386 -> initscore=-0.203150\n",
            "[LightGBM] [Info] Start training from score -0.203150\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 4/7 - LGBMClassifier\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 242220, number of negative: 297780\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107540 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3786\n",
            "[LightGBM] [Info] Number of data points in the train set: 540000, number of used features: 113\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.450066 -> initscore=-0.200403\n",
            "[LightGBM] [Info] Start training from score -0.200403\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 5/7 - LGBMClassifier\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 241969, number of negative: 298031\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121303 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3789\n",
            "[LightGBM] [Info] Number of data points in the train set: 540000, number of used features: 113\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.449593 -> initscore=-0.202313\n",
            "[LightGBM] [Info] Start training from score -0.202313\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 6/7 - LGBMClassifier\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 242143, number of negative: 297857\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114622 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3780\n",
            "[LightGBM] [Info] Number of data points in the train set: 540000, number of used features: 113\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.449882 -> initscore=-0.201149\n",
            "[LightGBM] [Info] Start training from score -0.201149\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 7/7 - LGBMClassifier\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 242323, number of negative: 297677\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.108785 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 3784\n",
            "[LightGBM] [Info] Number of data points in the train set: 540000, number of used features: 113\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.450265 -> initscore=-0.199602\n",
            "[LightGBM] [Info] Start training from score -0.199602\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 1/7 - HistGradientBoostingClassifier\n",
            "FOLD 2/7 - HistGradientBoostingClassifier\n",
            "FOLD 3/7 - HistGradientBoostingClassifier\n",
            "FOLD 4/7 - HistGradientBoostingClassifier\n",
            "FOLD 5/7 - HistGradientBoostingClassifier\n",
            "FOLD 6/7 - HistGradientBoostingClassifier\n",
            "FOLD 7/7 - HistGradientBoostingClassifier\n",
            "\n",
            "XGBClassifier OOF AUC: 0.955462\n",
            "XGBClassifier CV AUC mean: 0.955464, std: +-0.00043\n",
            "\n",
            "CatBoostClassifier OOF AUC: 0.955640\n",
            "CatBoostClassifier CV AUC mean: 0.955642, std: +-0.00043\n",
            "\n",
            "LGBMClassifier OOF AUC: 0.954186\n",
            "LGBMClassifier CV AUC mean: 0.954190, std: +-0.00048\n",
            "\n",
            "HistGradientBoostingClassifier OOF AUC: 0.955284\n",
            "HistGradientBoostingClassifier CV AUC mean: 0.955288, std: +-0.00045\n",
            "\n",
            "Stacked (XGB meta) OOF AUC: 0.955728\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import inspect\n",
        "\n",
        "SEED = 42\n",
        "NSPLITS = 7\n",
        "SEEDS_RUN = [42, 43]\n",
        "\n",
        "# Reload data Kaggle-style (lowercase columns, id as index)\n",
        "path = DATA_DIR\n",
        "dfs = []\n",
        "for fl in (\"train.csv\", \"test.csv\"):\n",
        "    df = pd.read_csv(path / fl, index_col=0)\n",
        "    df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns.tolist()]\n",
        "    dfs.append(df)\n",
        "train, test = dfs\n",
        "\n",
        "ystr = train.columns[-1]\n",
        "base_features = [c for c in train.columns if c != ystr]\n",
        "\n",
        "cols2comb = [\n",
        "    \"exercise_angina\", \"thallium\", \"chest_pain_type\",\n",
        "    \"slope_of_st\", \"sex\", \"st_depression\", \"number_of_vessels_fluro\",\n",
        "    \"ekg_results\", \"fbs_over_120\",\n",
        "]\n",
        "\n",
        "statmetrics = [\"mean\", \"count\"]\n",
        "\n",
        "X = train.drop(columns=ystr)\n",
        "y = (train[ystr] == \"Presence\").astype(int)\n",
        "\n",
        "X_test = test.copy()\n",
        "\n",
        "# Teacher model from original clinical dataset -> prior feature\n",
        "orig_path = DATA_DIR.parent / \"original-data\" / \"Heart_Disease_Prediction.csv\"\n",
        "orig_df = pd.read_csv(orig_path)\n",
        "orig_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in orig_df.columns]\n",
        "orig_ystr = orig_df.columns[-1]\n",
        "orig_X = orig_df.drop(columns=orig_ystr)\n",
        "orig_y = (orig_df[orig_ystr] == \"Presence\").astype(int)\n",
        "\n",
        "common_cols = sorted(set(orig_X.columns) & set(X.columns))\n",
        "print(f\"Teacher model using {len(common_cols)} shared columns from original dataset\")\n",
        "\n",
        "teacher = cb.CatBoostClassifier(\n",
        "    iterations=400,\n",
        "    depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.9,\n",
        "    colsample_bylevel=0.9,\n",
        "    random_seed=SEED,\n",
        "    verbose=0,\n",
        ")\n",
        "teacher.fit(orig_X[common_cols], orig_y)\n",
        "\n",
        "X[\"teacher_pred\"] = teacher.predict_proba(X[common_cols])[:, 1]\n",
        "X_test[\"teacher_pred\"] = teacher.predict_proba(X_test[common_cols])[:, 1]\n",
        "\n",
        "# Adversarial validation: train vs test\n",
        "adv_X = pd.concat([X, X_test], axis=0).reset_index(drop=True)\n",
        "adv_y = np.concatenate([\n",
        "    np.zeros(len(X), dtype=int),\n",
        "    np.ones(len(X_test), dtype=int),\n",
        "])\n",
        "adv_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "oof_adv = np.zeros(len(adv_y), dtype=float)\n",
        "for tr_adv, val_adv in adv_skf.split(adv_X, adv_y):\n",
        "    adv_clf = lgb.LGBMClassifier(\n",
        "        objective=\"binary\",\n",
        "        metric=\"auc\",\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=400,\n",
        "        num_leaves=31,\n",
        "        feature_fraction=0.9,\n",
        "        bagging_fraction=0.9,\n",
        "        bagging_freq=1,\n",
        "        min_data_in_leaf=30,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    adv_clf.fit(adv_X.iloc[tr_adv], adv_y[tr_adv])\n",
        "    oof_adv[val_adv] = adv_clf.predict_proba(adv_X.iloc[val_adv])[:, 1]\n",
        "\n",
        "auc_adv = roc_auc_score(adv_y, oof_adv)\n",
        "print(f\"Adversarial AUC (train vs test): {auc_adv:.5f}\")\n",
        "\n",
        "p_test_train = oof_adv[: len(X)]\n",
        "eps = 1e-3\n",
        "w_train = p_test_train / (1.0 - p_test_train + eps)\n",
        "w_train = w_train / w_train.mean()\n",
        "print(\n",
        "    f\"Sample weights (train-like vs test-like) -> min={w_train.min():.3f}, max={w_train.max():.3f}, mean={w_train.mean():.3f}\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_cat_feature_indices(X_):\n",
        "    return [i for i, c in enumerate(X_.columns) if c.startswith(\"CAT_\")]\n",
        "\n",
        "\n",
        "def fe_foldwise(X_tr, X_val, y_tr):\n",
        "    X_tr = X_tr.copy()\n",
        "    X_val = X_val.copy()\n",
        "\n",
        "    temp = pd.concat([X_tr, y_tr], axis=1)\n",
        "\n",
        "    # casting\n",
        "    for df in [X_tr, X_val]:\n",
        "        df[\"age>55\"] = (df[\"age\"] > 55).astype(int)\n",
        "        for col in df.columns:\n",
        "            if col == \"teacher_pred\":\n",
        "                continue\n",
        "            colname = f\"CAT_{col}\"\n",
        "            df[colname] = df[col].astype(str).astype(\"category\")\n",
        "\n",
        "    # numeric interactions and derived + bin features (from 0.954 notebook)\n",
        "    for df in [X_tr, X_val]:\n",
        "        df[\"chest_pain_type_bin\"] = (df[\"chest_pain_type\"] >= 3).astype(int)\n",
        "        df[\"st_depression_bin\"] = (df[\"st_depression\"] >= 2).astype(int)\n",
        "        df[\"number_of_vessels_fluro_bin\"] = (df[\"number_of_vessels_fluro\"] >= 2).astype(int)\n",
        "        df[\"hr_age\"] = df[\"max_hr\"] * df[\"age\"]\n",
        "        df[\"bp_age\"] = df[\"bp\"] * df[\"age\"]\n",
        "        df[\"st_slope\"] = df[\"st_depression\"] * df[\"slope_of_st\"]\n",
        "        df[\"chol_exercise\"] = df[\"cholesterol\"] * df[\"exercise_angina\"]\n",
        "        pred_max = (220 - df[\"age\"]).clip(lower=10)\n",
        "        df[\"max_hr_pct_pred\"] = df[\"max_hr\"] / pred_max\n",
        "        df[\"risk_sum\"] = df[\"number_of_vessels_fluro\"] + df[\"thallium\"] + df[\"exercise_angina\"]\n",
        "\n",
        "    # target statistics + smoothed target encoding\n",
        "    global_mean = float(y_tr.mean())\n",
        "    m_smooth = 20\n",
        "    for bf in base_features:\n",
        "        stats = temp.groupby(bf)[ystr].agg(statmetrics)\n",
        "        for s in statmetrics:\n",
        "            cname = f\"target_{bf}_{s}\"\n",
        "            X_tr[cname] = X_tr[bf].map(stats[s])\n",
        "            X_val[cname] = X_val[bf].map(stats[s])\n",
        "        smoothed = (stats[\"count\"] * stats[\"mean\"] + m_smooth * global_mean) / (stats[\"count\"] + m_smooth)\n",
        "        X_tr[f\"target_{bf}_smooth\"] = X_tr[bf].map(smoothed).fillna(global_mean)\n",
        "        X_val[f\"target_{bf}_smooth\"] = X_val[bf].map(smoothed).fillna(global_mean)\n",
        "\n",
        "    # categorical combinations\n",
        "    for i, c1 in enumerate(cols2comb[:-1]):\n",
        "        for c2 in cols2comb[i + 1 :]:\n",
        "            m2 = max(X_tr[c2].max(), X_val[c2].max()) + 1\n",
        "            cname = f\"{c1}_{c2}\"\n",
        "            X_tr[cname] = (\n",
        "                (X_tr[c1] + 1 + (X_tr[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "            X_val[cname] = (\n",
        "                (X_val[c1] + 1 + (X_val[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "\n",
        "    return X_tr, X_val\n",
        "\n",
        "\n",
        "def fe_test(X_test_, X_train_, y_train_):\n",
        "    X_test_ = X_test_.copy()\n",
        "    temp = pd.concat([X_train_, y_train_], axis=1)\n",
        "\n",
        "    X_test_[\"age>55\"] = (X_test_[\"age\"] > 55).astype(int)\n",
        "    for col in X_test_.columns:\n",
        "        if col == \"teacher_pred\":\n",
        "            continue\n",
        "        colname = f\"CAT_{col}\"\n",
        "        X_test_[colname] = X_test_[col].astype(str).astype(\"category\")\n",
        "\n",
        "    X_test_[\"chest_pain_type_bin\"] = (X_test_[\"chest_pain_type\"] >= 3).astype(int)\n",
        "    X_test_[\"st_depression_bin\"] = (X_test_[\"st_depression\"] >= 2).astype(int)\n",
        "    X_test_[\"number_of_vessels_fluro_bin\"] = (X_test_[\"number_of_vessels_fluro\"] >= 2).astype(int)\n",
        "    X_test_[\"hr_age\"] = X_test_[\"max_hr\"] * X_test_[\"age\"]\n",
        "    X_test_[\"bp_age\"] = X_test_[\"bp\"] * X_test_[\"age\"]\n",
        "    X_test_[\"st_slope\"] = X_test_[\"st_depression\"] * X_test_[\"slope_of_st\"]\n",
        "    X_test_[\"chol_exercise\"] = X_test_[\"cholesterol\"] * X_test_[\"exercise_angina\"]\n",
        "    pred_max = (220 - X_test_[\"age\"]).clip(lower=10)\n",
        "    X_test_[\"max_hr_pct_pred\"] = X_test_[\"max_hr\"] / pred_max\n",
        "    X_test_[\"risk_sum\"] = X_test_[\"number_of_vessels_fluro\"] + X_test_[\"thallium\"] + X_test_[\"exercise_angina\"]\n",
        "\n",
        "    global_mean = float(y_train_.mean())\n",
        "    m_smooth = 20\n",
        "    for bf in base_features:\n",
        "        stats = temp.groupby(bf)[ystr].agg(statmetrics)\n",
        "        for s in statmetrics:\n",
        "            X_test_[f\"target_{bf}_{s}\"] = X_test_[bf].map(stats[s])\n",
        "        smoothed = (stats[\"count\"] * stats[\"mean\"] + m_smooth * global_mean) / (stats[\"count\"] + m_smooth)\n",
        "        X_test_[f\"target_{bf}_smooth\"] = X_test_[bf].map(smoothed).fillna(global_mean)\n",
        "\n",
        "    for i, c1 in enumerate(cols2comb[:-1]):\n",
        "        for c2 in cols2comb[i + 1 :]:\n",
        "            m2 = max(X_train_[c2].max(), X_test_[c2].max()) + 1\n",
        "            cname = f\"{c1}_{c2}\"\n",
        "            X_test_[cname] = (\n",
        "                (X_test_[c1] + 1 + (X_test_[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "\n",
        "    return X_test_\n",
        "\n",
        "\n",
        "xgboost_params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"auc\",\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"max_depth\": 2,\n",
        "    \"subsample\": 0.9,\n",
        "    \"colsample_bytree\": 0.9,\n",
        "    \"n_estimators\": 5000,\n",
        "    \"min_child_weight\": 10,\n",
        "    \"gamma\": 1,\n",
        "    \"reg_lambda\": 0.01,\n",
        "    \"reg_alpha\": 1.5,\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": SEED,\n",
        "    \"early_stopping_rounds\": 300,\n",
        "    \"enable_categorical\": True,\n",
        "}\n",
        "\n",
        "catboost_params = {\n",
        "    \"loss_function\": \"Logloss\",\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"depth\": 2,\n",
        "    \"subsample\": 0.9,\n",
        "    \"iterations\": 5000,\n",
        "    \"min_data_in_leaf\": 1,\n",
        "    \"l2_leaf_reg\": 1.002,\n",
        "    \"thread_count\": -1,\n",
        "    \"random_seed\": SEED,\n",
        "    \"early_stopping_rounds\": 300,\n",
        "    \"bootstrap_type\": \"Bernoulli\",\n",
        "}\n",
        "\n",
        "lgbm_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"n_estimators\": 4000,\n",
        "    \"num_leaves\": 31,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.9,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"min_data_in_leaf\": 30,\n",
        "    \"random_state\": SEED,\n",
        "    \"n_jobs\": -1,\n",
        "}\n",
        "\n",
        "hgb_params = {\n",
        "    \"max_iter\": 5000,\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"max_depth\": 6,\n",
        "    \"early_stopping\": True,\n",
        "    \"n_iter_no_change\": 80,\n",
        "    \"validation_fraction\": 0.1,\n",
        "    \"random_state\": SEED,\n",
        "}\n",
        "\n",
        "models = {\n",
        "    XGBClassifier: xgboost_params,\n",
        "    CatBoostClassifier: catboost_params,\n",
        "    LGBMClassifier: lgbm_params,\n",
        "    HistGradientBoostingClassifier: hgb_params,\n",
        "}\n",
        "\n",
        "kf = KFold(n_splits=NSPLITS, shuffle=True, random_state=SEED)\n",
        "\n",
        "oof_train_model = {}\n",
        "oof_test_model = {}\n",
        "cv_auc_model = defaultdict(list)\n",
        "\n",
        "for modelClass, param in models.items():\n",
        "    model_name = modelClass.__name__\n",
        "    oof_train = np.zeros(len(X))\n",
        "    oof_test = np.zeros(len(X_test))\n",
        "\n",
        "    for fold, (tr, val) in enumerate(kf.split(X)):\n",
        "        print(f\"FOLD {fold + 1}/{NSPLITS} - {model_name}\")\n",
        "\n",
        "        X_tr_raw, X_val_raw = X.iloc[tr], X.iloc[val]\n",
        "        y_tr, y_val = y.iloc[tr], y.iloc[val]\n",
        "\n",
        "        X_tr, X_val = fe_foldwise(X_tr_raw, X_val_raw, y_tr)\n",
        "\n",
        "        model = modelClass(**param)\n",
        "        if model_name == \"HistGradientBoostingClassifier\":\n",
        "            X_tr_fit = X_tr.select_dtypes(include=[np.number])\n",
        "            X_val_fit = X_val.select_dtypes(include=[np.number])\n",
        "            fit_kwargs = {\"X\": X_tr_fit, \"y\": y_tr, \"sample_weight\": w_train[tr]}\n",
        "            model.fit(**fit_kwargs)\n",
        "            oof_train[val] = model.predict_proba(X_val_fit)[:, 1]\n",
        "            X_test_fe = fe_test(X_test, X_tr_raw, y_tr)\n",
        "            X_test_fit = X_test_fe.select_dtypes(include=[np.number])\n",
        "            oof_test += model.predict_proba(X_test_fit)[:, 1] / NSPLITS\n",
        "        else:\n",
        "            fit_kwargs = {\n",
        "                \"X\": X_tr,\n",
        "                \"y\": y_tr,\n",
        "                \"eval_set\": [(X_val, y_val)],\n",
        "                \"sample_weight\": w_train[tr],\n",
        "            }\n",
        "            if model_name != \"LGBMClassifier\":\n",
        "                fit_kwargs[\"verbose\"] = 2000\n",
        "            if \"cat_features\" in inspect.signature(model.fit).parameters:\n",
        "                cat_features = get_cat_feature_indices(X_tr)\n",
        "                fit_kwargs[\"cat_features\"] = cat_features\n",
        "            model.fit(**fit_kwargs)\n",
        "            oof_train[val] = model.predict_proba(X_val)[:, 1]\n",
        "            X_test_fe = fe_test(X_test, X_tr_raw, y_tr)\n",
        "            oof_test += model.predict_proba(X_test_fe)[:, 1] / NSPLITS\n",
        "        cv_auc_model[model_name].append(roc_auc_score(y[val], oof_train[val]))\n",
        "\n",
        "    oof_train_model[model_name] = oof_train\n",
        "    oof_test_model[model_name] = oof_test\n",
        "\n",
        "# Evaluation per model\n",
        "for modelClass in models.keys():\n",
        "    model_name = modelClass.__name__\n",
        "    print(f\"\\n{model_name} OOF AUC: {roc_auc_score(y, oof_train_model[model_name]):.6f}\")\n",
        "    print(\n",
        "        f\"{model_name} CV AUC mean: {np.mean(cv_auc_model[model_name]):.6f}, std: +-{np.std(cv_auc_model[model_name]):.5f}\"\n",
        "    )\n",
        "\n",
        "# Stacking: XGB meta-learner (non-linear combination of 3 base OOF predictions)\n",
        "X_oof_tr = pd.DataFrame.from_dict(oof_train_model)\n",
        "X_oof_test = pd.DataFrame.from_dict(oof_test_model)\n",
        "\n",
        "meta = XGBClassifier(max_depth=2, n_estimators=150, learning_rate=0.1, subsample=0.8, colsample_bytree=0.8, random_state=SEED, eval_metric=\"auc\")\n",
        "meta.fit(X_oof_tr, y, sample_weight=w_train)\n",
        "\n",
        "oof_tr_final = pd.Series(meta.predict_proba(X_oof_tr)[:, 1], index=X_oof_tr.index)\n",
        "oof_test_final = pd.Series(meta.predict_proba(X_oof_test)[:, 1], index=X_oof_test.index)\n",
        "stack_auc = roc_auc_score(y, oof_tr_final)\n",
        "print(f\"\\nStacked (XGB meta) OOF AUC: {stack_auc:.6f}\")\n",
        "\n",
        "# Set variables used by later cells\n",
        "oof = oof_tr_final.values\n",
        "test_proba = oof_test_final.values\n",
        "\n",
        "# Ensure id column exists for submission\n",
        "test[\"id\"] = test.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix (OOF, threshold=0.5)\n",
            "Rows: true, Cols: predicted |  Absence   Presence\n",
            "[[314123  33423]\n",
            " [ 36351 246103]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "y_pred = (oof >= 0.5).astype(int)\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "print(\"Confusion matrix (OOF, threshold=0.5)\")\n",
        "print(\"Rows: true, Cols: predicted |  Absence   Presence\")\n",
        "print(cm)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(cm, cmap=\"Blues\")\n",
        "plt.colorbar()\n",
        "plt.xticks([0, 1], [\"Absence\", \"Presence\"])\n",
        "plt.yticks([0, 1], [\"Absence\", \"Presence\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
        "plt.title(\"Confusion matrix (OOF)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission saved to submission.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Heart Disease</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>630000</th>\n",
              "      <td>630000</td>\n",
              "      <td>0.945500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630001</th>\n",
              "      <td>630001</td>\n",
              "      <td>0.008632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630002</th>\n",
              "      <td>630002</td>\n",
              "      <td>0.988417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630003</th>\n",
              "      <td>630003</td>\n",
              "      <td>0.004278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630004</th>\n",
              "      <td>630004</td>\n",
              "      <td>0.205854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630005</th>\n",
              "      <td>630005</td>\n",
              "      <td>0.985589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630006</th>\n",
              "      <td>630006</td>\n",
              "      <td>0.005370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630007</th>\n",
              "      <td>630007</td>\n",
              "      <td>0.541404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630008</th>\n",
              "      <td>630008</td>\n",
              "      <td>0.992537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630009</th>\n",
              "      <td>630009</td>\n",
              "      <td>0.011532</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  Heart Disease\n",
              "id                           \n",
              "630000  630000       0.945500\n",
              "630001  630001       0.008632\n",
              "630002  630002       0.988417\n",
              "630003  630003       0.004278\n",
              "630004  630004       0.205854\n",
              "630005  630005       0.985589\n",
              "630006  630006       0.005370\n",
              "630007  630007       0.541404\n",
              "630008  630008       0.992537\n",
              "630009  630009       0.011532"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub = pd.DataFrame({\"id\": test[\"id\"], \"Heart Disease\": test_proba})\n",
        "sub.to_csv(OUTPUT_DIR / \"submission.csv\", index=False)\n",
        "print(f\"Submission saved to {OUTPUT_DIR / 'submission.csv'}\")\n",
        "sub.head(10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
