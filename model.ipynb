{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting Heart Disease\n",
        "\n",
        "## Score: .95373"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict, KFold\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "import catboost as cb\n",
        "from catboost import CatBoostClassifier\n",
        "#%pip install torch\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DATA_DIR = Path(\"playground-series-s6e2\")\n",
        "OUTPUT_DIR = Path(\".\")\n",
        "n_splits = 5\n",
        "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (630000, 15)\n",
            "Test: (270000, 14)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Chest pain type</th>\n",
              "      <th>BP</th>\n",
              "      <th>Cholesterol</th>\n",
              "      <th>FBS over 120</th>\n",
              "      <th>EKG results</th>\n",
              "      <th>Max HR</th>\n",
              "      <th>Exercise angina</th>\n",
              "      <th>ST depression</th>\n",
              "      <th>Slope of ST</th>\n",
              "      <th>Number of vessels fluro</th>\n",
              "      <th>Thallium</th>\n",
              "      <th>Heart Disease</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>152</td>\n",
              "      <td>239</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>158</td>\n",
              "      <td>1</td>\n",
              "      <td>3.6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>Presence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>325</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>171</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>160</td>\n",
              "      <td>188</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>134</td>\n",
              "      <td>229</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Absence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>58</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>140</td>\n",
              "      <td>234</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>3.8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Presence</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n",
              "0   0   58    1                4  152          239             0            0   \n",
              "1   1   52    1                1  125          325             0            2   \n",
              "2   2   56    0                2  160          188             0            2   \n",
              "3   3   44    0                3  134          229             0            2   \n",
              "4   4   58    1                4  140          234             0            2   \n",
              "\n",
              "   Max HR  Exercise angina  ST depression  Slope of ST  \\\n",
              "0     158                1            3.6            2   \n",
              "1     171                0            0.0            1   \n",
              "2     151                0            0.0            1   \n",
              "3     150                0            1.0            2   \n",
              "4     125                1            3.8            2   \n",
              "\n",
              "   Number of vessels fluro  Thallium Heart Disease  \n",
              "0                        2         7      Presence  \n",
              "1                        0         3       Absence  \n",
              "2                        0         3       Absence  \n",
              "3                        0         3       Absence  \n",
              "4                        3         3      Presence  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
        "test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
        "print(f\"Train: {train.shape}\")\n",
        "print(f\"Test: {test.shape}\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
            "[WinError 2] The system cannot find the file specified\n",
            "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
            "  warnings.warn(\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
            "    cpu_info = subprocess.run(\n",
            "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
            "        capture_output=True,\n",
            "        text=True,\n",
            "    )\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 554, in run\n",
            "    with Popen(*popenargs, **kwargs) as process:\n",
            "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1039, in __init__\n",
            "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
            "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "                        pass_fds, cwd, env,\n",
            "                        ^^^^^^^^^^^^^^^^^^^\n",
            "    ...<5 lines>...\n",
            "                        gid, gids, uid, umask,\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
            "                        start_new_session, process_group)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\ol1v3_7dwns5u\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
            "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
            "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
            "                             # no special security\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^\n",
            "    ...<4 lines>...\n",
            "                             cwd,\n",
            "                             ^^^^\n",
            "                             startupinfo)\n",
            "                             ^^^^^^^^^^^^\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: 32 columns\n",
            "Target distribution: {0: 347546, 1: 282454}\n"
          ]
        }
      ],
      "source": [
        "target_col = \"Heart Disease\"\n",
        "id_col = \"id\"\n",
        "feature_cols = [c for c in train.columns if c not in (id_col, target_col)]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(train[target_col])\n",
        "\n",
        "X_train = train[feature_cols].copy()\n",
        "X_test = test[feature_cols].copy()\n",
        "\n",
        "for col in feature_cols:\n",
        "    if X_train[col].isna().any() or X_test[col].isna().any():\n",
        "        med = X_train[col].median()\n",
        "        X_train[col] = X_train[col].fillna(med)\n",
        "        X_test[col] = X_test[col].fillna(med)\n",
        "\n",
        "X_train[\"chol_exercise\"] = X_train[\"Cholesterol\"] * X_train[\"Exercise angina\"]\n",
        "X_test[\"chol_exercise\"] = X_test[\"Cholesterol\"] * X_test[\"Exercise angina\"]\n",
        "X_train[\"st_slope\"] = X_train[\"ST depression\"] * X_train[\"Slope of ST\"]\n",
        "X_test[\"st_slope\"] = X_test[\"ST depression\"] * X_test[\"Slope of ST\"]\n",
        "X_train[\"hr_age\"] = X_train[\"Max HR\"] * X_train[\"Age\"]\n",
        "X_test[\"hr_age\"] = X_test[\"Max HR\"] * X_test[\"Age\"]\n",
        "X_train[\"bp_age\"] = X_train[\"BP\"] * X_train[\"Age\"]\n",
        "X_test[\"bp_age\"] = X_test[\"BP\"] * X_test[\"Age\"]\n",
        "\n",
        "te_cols = [\"Chest pain type\", \"Slope of ST\", \"Thallium\"]\n",
        "global_mean = float(y.mean())\n",
        "m = 20\n",
        "for col in te_cols:\n",
        "    agg = pd.DataFrame({\"_y\": y}).groupby(X_train[col])[\"_y\"].agg([\"mean\", \"count\"])\n",
        "    smoothed = (agg[\"count\"] * agg[\"mean\"] + m * global_mean) / (agg[\"count\"] + m)\n",
        "    X_train[col + \"_te\"] = X_train[col].map(smoothed).fillna(global_mean)\n",
        "    X_test[col + \"_te\"] = X_test[col].map(smoothed).fillna(global_mean)\n",
        "\n",
        "scaler_feat = StandardScaler()\n",
        "X_tr_s = scaler_feat.fit_transform(X_train)\n",
        "X_te_s = scaler_feat.transform(X_test)\n",
        "kmeans = KMeans(n_clusters=12, random_state=42, n_init=10)\n",
        "kmeans.fit(X_tr_s)\n",
        "for i in range(kmeans.n_clusters):\n",
        "    d_tr = np.linalg.norm(X_tr_s - kmeans.cluster_centers_[i], axis=1)\n",
        "    d_te = np.linalg.norm(X_te_s - kmeans.cluster_centers_[i], axis=1)\n",
        "    X_train[f\"dist_c{i}\"] = d_tr\n",
        "    X_test[f\"dist_c{i}\"] = d_te\n",
        "\n",
        "print(f\"Features: {len(X_train.columns)} columns\")\n",
        "print(f\"Target distribution: {pd.Series(y).value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best OOF AUC: 0.95507, params: {'depth': 6, 'lr': 0.05, 'min_data_in_leaf': 15}\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\"depth\": [4, 5, 6], \"lr\": [0.03, 0.04, 0.05], \"min_data_in_leaf\": [15, 25, 35]}\n",
        "n_est = 800\n",
        "best_auc, best_params = 0, None\n",
        "for depth in param_grid[\"depth\"]:\n",
        "    for lr in param_grid[\"lr\"]:\n",
        "        for min_leaf in param_grid[\"min_data_in_leaf\"]:\n",
        "            m = cb.CatBoostClassifier(iterations=n_est, depth=depth, learning_rate=lr, min_data_in_leaf=min_leaf, subsample=0.75, colsample_bylevel=0.75, random_seed=42, verbose=0)\n",
        "            oof = cross_val_predict(m, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "            auc = roc_auc_score(y, oof)\n",
        "            if auc > best_auc:\n",
        "                best_auc, best_params = auc, {\"depth\": depth, \"lr\": lr, \"min_data_in_leaf\": min_leaf}\n",
        "print(f\"Best OOF AUC: {best_auc:.5f}, params: {best_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CV AUC: 0.95517 (blend w_xgb=0.4)\n"
          ]
        }
      ],
      "source": [
        "model = cb.CatBoostClassifier(iterations=n_est, depth=best_params[\"depth\"], learning_rate=best_params[\"lr\"], min_data_in_leaf=best_params[\"min_data_in_leaf\"], subsample=0.75, colsample_bylevel=0.75, random_seed=42, verbose=0)\n",
        "oof_cb = cross_val_predict(model, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=n_est, max_depth=5, learning_rate=0.05, min_child_weight=20, subsample=0.75, colsample_bytree=0.75, random_state=42, eval_metric=\"auc\")\n",
        "oof_xgb = cross_val_predict(xgb_model, X_train, y, cv=cv, method=\"predict_proba\")[:, 1]\n",
        "best_w, best_auc = 0.0, roc_auc_score(y, oof_cb)\n",
        "for w in [0, 0.1, 0.2, 0.3, 0.4]:\n",
        "    oof_blend = (1 - w) * oof_cb + w * oof_xgb\n",
        "    auc = roc_auc_score(y, oof_blend)\n",
        "    if auc > best_auc:\n",
        "        best_auc, best_w = auc, w\n",
        "oof = (1 - best_w) * oof_cb + best_w * oof_xgb\n",
        "print(f\"CV AUC: {best_auc:.5f} (blend w_xgb={best_w})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, y)\n",
        "xgb_model.fit(X_train, y)\n",
        "test_cb = model.predict_proba(X_test)[:, 1]\n",
        "test_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "test_proba = (1 - best_w) * test_cb + best_w * test_xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Teacher model using 13 shared columns from original dataset\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010548 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 668\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010582 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 672\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011040 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 671\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "Adversarial AUC (train vs test): 0.50147\n",
            "Sample weights (train-like vs test-like) -> min=0.350, max=2.793, mean=1.000\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010744 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 668\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010166 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 672\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Info] Number of positive: 180000, number of negative: 420000\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009664 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 671\n",
            "[LightGBM] [Info] Number of data points in the train set: 600000, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.300000 -> initscore=-0.847298\n",
            "[LightGBM] [Info] Start training from score -0.847298\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=30, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=30\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "FOLD 1/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90516\n",
            "[1999]\tvalidation_0-auc:0.95548\n",
            "FOLD 2/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90529\n",
            "[1999]\tvalidation_0-auc:0.95525\n",
            "FOLD 3/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90522\n",
            "[1945]\tvalidation_0-auc:0.95526\n",
            "FOLD 4/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90853\n",
            "[1999]\tvalidation_0-auc:0.95613\n",
            "FOLD 5/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90700\n",
            "[1999]\tvalidation_0-auc:0.95584\n",
            "FOLD 6/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90400\n",
            "[1999]\tvalidation_0-auc:0.95466\n",
            "FOLD 7/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.90582\n",
            "[1943]\tvalidation_0-auc:0.95554\n",
            "FOLD 1/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713129\ttest: 0.6713563\tbest: 0.6713563 (0)\ttotal: 127ms\tremaining: 4m 13s\n",
            "1999:\tlearn: 0.2674361\ttest: 0.2673850\tbest: 0.2673850 (1998)\ttotal: 3m 33s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2673850295\n",
            "bestIteration = 1998\n",
            "\n",
            "Shrink model to first 1999 iterations.\n",
            "FOLD 2/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713393\ttest: 0.6712824\tbest: 0.6712824 (0)\ttotal: 112ms\tremaining: 3m 43s\n",
            "1999:\tlearn: 0.2672932\ttest: 0.2681611\tbest: 0.2681611 (1999)\ttotal: 3m 54s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2681610726\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 3/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713336\ttest: 0.6713670\tbest: 0.6713670 (0)\ttotal: 112ms\tremaining: 3m 44s\n",
            "1999:\tlearn: 0.2673590\ttest: 0.2677273\tbest: 0.2677273 (1999)\ttotal: 3m 38s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.267727319\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 4/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713737\ttest: 0.6712359\tbest: 0.6712359 (0)\ttotal: 116ms\tremaining: 3m 51s\n",
            "1999:\tlearn: 0.2678315\ttest: 0.2652450\tbest: 0.2652450 (1999)\ttotal: 3m 33s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2652449668\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 5/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713705\ttest: 0.6712575\tbest: 0.6712575 (0)\ttotal: 126ms\tremaining: 4m 12s\n",
            "1999:\tlearn: 0.2676781\ttest: 0.2661560\tbest: 0.2661560 (1999)\ttotal: 3m 34s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2661560457\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 6/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713353\ttest: 0.6713303\tbest: 0.6713303 (0)\ttotal: 122ms\tremaining: 4m 4s\n",
            "1999:\tlearn: 0.2670949\ttest: 0.2700115\tbest: 0.2700115 (1999)\ttotal: 3m 35s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2700115322\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 7/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713651\ttest: 0.6712699\tbest: 0.6712699 (0)\ttotal: 113ms\tremaining: 3m 45s\n",
            "1999:\tlearn: 0.2675060\ttest: 0.2668935\tbest: 0.2668935 (1999)\ttotal: 4m 1s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2668935161\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 1/7 - LGBMClassifier\n",
            "FOLD 2/7 - LGBMClassifier\n",
            "FOLD 3/7 - LGBMClassifier\n",
            "FOLD 4/7 - LGBMClassifier\n",
            "FOLD 5/7 - LGBMClassifier\n",
            "FOLD 6/7 - LGBMClassifier\n",
            "FOLD 7/7 - LGBMClassifier\n",
            "FOLD 1/7 - HistGradientBoostingClassifier\n",
            "FOLD 2/7 - HistGradientBoostingClassifier\n",
            "FOLD 3/7 - HistGradientBoostingClassifier\n",
            "FOLD 4/7 - HistGradientBoostingClassifier\n",
            "FOLD 5/7 - HistGradientBoostingClassifier\n",
            "FOLD 6/7 - HistGradientBoostingClassifier\n",
            "FOLD 7/7 - HistGradientBoostingClassifier\n",
            "\n",
            "XGBClassifier OOF AUC: 0.955449\n",
            "XGBClassifier CV AUC mean: 0.955451, std: +-0.00043\n",
            "\n",
            "CatBoostClassifier OOF AUC: 0.955561\n",
            "CatBoostClassifier CV AUC mean: 0.955563, std: +-0.00044\n",
            "\n",
            "LGBMClassifier OOF AUC: 0.954954\n",
            "LGBMClassifier CV AUC mean: 0.954958, std: +-0.00046\n",
            "\n",
            "HistGradientBoostingClassifier OOF AUC: 0.955284\n",
            "HistGradientBoostingClassifier CV AUC mean: 0.955288, std: +-0.00045\n",
            "\n",
            "Stack (LR meta, 4 models) OOF AUC: 0.955558\n",
            "FOLD 1/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.89157\n",
            "[1999]\tvalidation_0-auc:0.95573\n",
            "FOLD 2/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.89217\n",
            "[1872]\tvalidation_0-auc:0.95502\n",
            "FOLD 3/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.89022\n",
            "[1999]\tvalidation_0-auc:0.95616\n",
            "FOLD 4/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.89124\n",
            "[1922]\tvalidation_0-auc:0.95552\n",
            "FOLD 5/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.88994\n",
            "[1999]\tvalidation_0-auc:0.95530\n",
            "FOLD 6/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.89067\n",
            "[1999]\tvalidation_0-auc:0.95449\n",
            "FOLD 7/7 - XGBClassifier\n",
            "[0]\tvalidation_0-auc:0.89054\n",
            "[1999]\tvalidation_0-auc:0.95590\n",
            "FOLD 1/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713961\ttest: 0.6713107\tbest: 0.6713107 (0)\ttotal: 118ms\tremaining: 3m 56s\n",
            "1999:\tlearn: 0.2676079\ttest: 0.2667323\tbest: 0.2667323 (1999)\ttotal: 3m 30s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2667323181\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 2/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713550\ttest: 0.6713811\tbest: 0.6713811 (0)\ttotal: 129ms\tremaining: 4m 18s\n",
            "1999:\tlearn: 0.2672005\ttest: 0.2688390\tbest: 0.2688377 (1998)\ttotal: 3m 47s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.268837738\n",
            "bestIteration = 1998\n",
            "\n",
            "Shrink model to first 1999 iterations.\n",
            "FOLD 3/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714103\ttest: 0.6712690\tbest: 0.6712690 (0)\ttotal: 135ms\tremaining: 4m 29s\n",
            "1999:\tlearn: 0.2678239\ttest: 0.2653877\tbest: 0.2653864 (1997)\ttotal: 4m 12s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2653863761\n",
            "bestIteration = 1997\n",
            "\n",
            "Shrink model to first 1998 iterations.\n",
            "FOLD 4/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713976\ttest: 0.6713194\tbest: 0.6713194 (0)\ttotal: 148ms\tremaining: 4m 56s\n",
            "1999:\tlearn: 0.2675938\ttest: 0.2668366\tbest: 0.2668366 (1999)\ttotal: 3m 58s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2668366462\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 5/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713242\ttest: 0.6714190\tbest: 0.6714190 (0)\ttotal: 136ms\tremaining: 4m 31s\n",
            "1999:\tlearn: 0.2673913\ttest: 0.2680567\tbest: 0.2680567 (1999)\ttotal: 4m 2s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2680567373\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 6/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6713713\ttest: 0.6713634\tbest: 0.6713634 (0)\ttotal: 139ms\tremaining: 4m 38s\n",
            "1999:\tlearn: 0.2670833\ttest: 0.2701009\tbest: 0.2701009 (1999)\ttotal: 4m 6s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2701008773\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 7/7 - CatBoostClassifier\n",
            "0:\tlearn: 0.6714243\ttest: 0.6712464\tbest: 0.6712464 (0)\ttotal: 163ms\tremaining: 5m 25s\n",
            "1999:\tlearn: 0.2677278\ttest: 0.2659659\tbest: 0.2659659 (1999)\ttotal: 4m 7s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.2659659307\n",
            "bestIteration = 1999\n",
            "\n",
            "FOLD 1/7 - LGBMClassifier\n",
            "FOLD 2/7 - LGBMClassifier\n",
            "FOLD 3/7 - LGBMClassifier\n",
            "FOLD 4/7 - LGBMClassifier\n",
            "FOLD 5/7 - LGBMClassifier\n",
            "FOLD 6/7 - LGBMClassifier\n",
            "FOLD 7/7 - LGBMClassifier\n",
            "FOLD 1/7 - HistGradientBoostingClassifier\n",
            "FOLD 2/7 - HistGradientBoostingClassifier\n",
            "FOLD 3/7 - HistGradientBoostingClassifier\n",
            "FOLD 4/7 - HistGradientBoostingClassifier\n",
            "FOLD 5/7 - HistGradientBoostingClassifier\n",
            "FOLD 6/7 - HistGradientBoostingClassifier\n",
            "FOLD 7/7 - HistGradientBoostingClassifier\n",
            "\n",
            "XGBClassifier OOF AUC: 0.955443\n",
            "XGBClassifier CV AUC mean: 0.955447, std: +-0.00053\n",
            "\n",
            "CatBoostClassifier OOF AUC: 0.955545\n",
            "CatBoostClassifier CV AUC mean: 0.955548, std: +-0.00052\n",
            "\n",
            "LGBMClassifier OOF AUC: 0.954960\n",
            "LGBMClassifier CV AUC mean: 0.954964, std: +-0.00052\n",
            "\n",
            "HistGradientBoostingClassifier OOF AUC: 0.955274\n",
            "HistGradientBoostingClassifier CV AUC mean: 0.955277, std: +-0.00054\n",
            "\n",
            "Stack (LR meta, 4 models) OOF AUC: 0.955551\n",
            "Submission: 4-model stack, 2-seed avg. test_proba shape: (270000,)\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import inspect\n",
        "\n",
        "SEED = 42\n",
        "NSPLITS = 7\n",
        "SEEDS = [42, 43]  # riskier: seed averaging (2 runs, average predictions)\n",
        "USE_LR_STACK = False\n",
        "DROP_BP_MAX_HR = True\n",
        "\n",
        "# Reload data Kaggle-style (lowercase columns, id as index)\n",
        "path = DATA_DIR\n",
        "dfs = []\n",
        "for fl in (\"train.csv\", \"test.csv\"):\n",
        "    df = pd.read_csv(path / fl, index_col=0)\n",
        "    df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns.tolist()]\n",
        "    dfs.append(df)\n",
        "train, test = dfs\n",
        "\n",
        "ystr = train.columns[-1]\n",
        "base_features = [c for c in train.columns if c != ystr]\n",
        "\n",
        "cols2comb = [\n",
        "    \"exercise_angina\", \"thallium\", \"chest_pain_type\",\n",
        "    \"slope_of_st\", \"sex\", \"st_depression\", \"number_of_vessels_fluro\",\n",
        "    \"ekg_results\", \"fbs_over_120\",\n",
        "]\n",
        "\n",
        "statmetrics = [\"mean\", \"count\"]\n",
        "\n",
        "X = train.drop(columns=ystr)\n",
        "y = (train[ystr] == \"Presence\").astype(int)\n",
        "\n",
        "X_test = test.copy()\n",
        "\n",
        "# Teacher model from original clinical dataset -> prior feature\n",
        "orig_path = DATA_DIR.parent / \"original-data\" / \"Heart_Disease_Prediction.csv\"\n",
        "orig_df = pd.read_csv(orig_path)\n",
        "orig_df.columns = [c.strip().lower().replace(\" \", \"_\") for c in orig_df.columns]\n",
        "orig_ystr = orig_df.columns[-1]\n",
        "orig_X = orig_df.drop(columns=orig_ystr)\n",
        "orig_y = (orig_df[orig_ystr] == \"Presence\").astype(int)\n",
        "\n",
        "common_cols = sorted(set(orig_X.columns) & set(X.columns))\n",
        "print(f\"Teacher model using {len(common_cols)} shared columns from original dataset\")\n",
        "\n",
        "teacher = cb.CatBoostClassifier(\n",
        "    iterations=400,\n",
        "    depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.9,\n",
        "    colsample_bylevel=0.9,\n",
        "    random_seed=SEED,\n",
        "    verbose=0,\n",
        ")\n",
        "teacher.fit(orig_X[common_cols], orig_y)\n",
        "\n",
        "X[\"teacher_pred\"] = teacher.predict_proba(X[common_cols])[:, 1]\n",
        "X_test[\"teacher_pred\"] = teacher.predict_proba(X_test[common_cols])[:, 1]\n",
        "\n",
        "# Adversarial validation: train vs test\n",
        "adv_X = pd.concat([X, X_test], axis=0).reset_index(drop=True)\n",
        "adv_y = np.concatenate([\n",
        "    np.zeros(len(X), dtype=int),\n",
        "    np.ones(len(X_test), dtype=int),\n",
        "])\n",
        "adv_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "oof_adv = np.zeros(len(adv_y), dtype=float)\n",
        "for tr_adv, val_adv in adv_skf.split(adv_X, adv_y):\n",
        "    adv_clf = lgb.LGBMClassifier(\n",
        "        objective=\"binary\",\n",
        "        metric=\"auc\",\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=400,\n",
        "        num_leaves=31,\n",
        "        feature_fraction=0.9,\n",
        "        bagging_fraction=0.9,\n",
        "        bagging_freq=1,\n",
        "        min_data_in_leaf=30,\n",
        "        random_state=SEED,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    adv_clf.fit(adv_X.iloc[tr_adv], adv_y[tr_adv])\n",
        "    oof_adv[val_adv] = adv_clf.predict_proba(adv_X.iloc[val_adv])[:, 1]\n",
        "\n",
        "auc_adv = roc_auc_score(adv_y, oof_adv)\n",
        "print(f\"Adversarial AUC (train vs test): {auc_adv:.5f}\")\n",
        "\n",
        "p_test_train = oof_adv[: len(X)]\n",
        "eps = 1e-3\n",
        "w_train = p_test_train / (1.0 - p_test_train + eps)\n",
        "w_train = w_train / w_train.mean()\n",
        "print(\n",
        "    f\"Sample weights (train-like vs test-like) -> min={w_train.min():.3f}, max={w_train.max():.3f}, mean={w_train.mean():.3f}\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_cat_feature_indices(X_):\n",
        "    return [i for i, c in enumerate(X_.columns) if c.startswith(\"CAT_\")]\n",
        "\n",
        "\n",
        "def fe_foldwise(X_tr, X_val, y_tr):\n",
        "    X_tr = X_tr.copy()\n",
        "    X_val = X_val.copy()\n",
        "\n",
        "    temp = pd.concat([X_tr, y_tr], axis=1)\n",
        "\n",
        "    # casting\n",
        "    for df in [X_tr, X_val]:\n",
        "        df[\"age>55\"] = (df[\"age\"] > 55).astype(int)\n",
        "        for col in df.columns:\n",
        "            if col == \"teacher_pred\":\n",
        "                continue\n",
        "            colname = f\"CAT_{col}\"\n",
        "            df[colname] = df[col].astype(str).astype(\"category\")\n",
        "\n",
        "    # numeric interactions and derived + bin features (from 0.954 notebook)\n",
        "    for df in [X_tr, X_val]:\n",
        "        df[\"chest_pain_type_bin\"] = (df[\"chest_pain_type\"] >= 3).astype(int)\n",
        "        df[\"st_depression_bin\"] = (df[\"st_depression\"] >= 2).astype(int)\n",
        "        df[\"number_of_vessels_fluro_bin\"] = (df[\"number_of_vessels_fluro\"] >= 2).astype(int)\n",
        "        df[\"hr_age\"] = df[\"max_hr\"] * df[\"age\"]\n",
        "        df[\"bp_age\"] = df[\"bp\"] * df[\"age\"]\n",
        "        df[\"st_slope\"] = df[\"st_depression\"] * df[\"slope_of_st\"]\n",
        "        df[\"chol_exercise\"] = df[\"cholesterol\"] * df[\"exercise_angina\"]\n",
        "        pred_max = (220 - df[\"age\"]).clip(lower=10)\n",
        "        df[\"max_hr_pct_pred\"] = df[\"max_hr\"] / pred_max\n",
        "        df[\"risk_sum\"] = df[\"number_of_vessels_fluro\"] + df[\"thallium\"] + df[\"exercise_angina\"]\n",
        "\n",
        "    # target statistics + smoothed target encoding\n",
        "    global_mean = float(y_tr.mean())\n",
        "    m_smooth = 20\n",
        "    for bf in base_features:\n",
        "        stats = temp.groupby(bf)[ystr].agg(statmetrics)\n",
        "        for s in statmetrics:\n",
        "            cname = f\"target_{bf}_{s}\"\n",
        "            X_tr[cname] = X_tr[bf].map(stats[s])\n",
        "            X_val[cname] = X_val[bf].map(stats[s])\n",
        "        smoothed = (stats[\"count\"] * stats[\"mean\"] + m_smooth * global_mean) / (stats[\"count\"] + m_smooth)\n",
        "        X_tr[f\"target_{bf}_smooth\"] = X_tr[bf].map(smoothed).fillna(global_mean)\n",
        "        X_val[f\"target_{bf}_smooth\"] = X_val[bf].map(smoothed).fillna(global_mean)\n",
        "\n",
        "    # categorical combinations\n",
        "    for i, c1 in enumerate(cols2comb[:-1]):\n",
        "        for c2 in cols2comb[i + 1 :]:\n",
        "            m2 = max(X_tr[c2].max(), X_val[c2].max()) + 1\n",
        "            cname = f\"{c1}_{c2}\"\n",
        "            X_tr[cname] = (\n",
        "                (X_tr[c1] + 1 + (X_tr[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "            X_val[cname] = (\n",
        "                (X_val[c1] + 1 + (X_val[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "\n",
        "    if DROP_BP_MAX_HR:\n",
        "        X_tr = X_tr.drop(columns=[\"bp\", \"max_hr\"], errors=\"ignore\")\n",
        "        X_val = X_val.drop(columns=[\"bp\", \"max_hr\"], errors=\"ignore\")\n",
        "    return X_tr, X_val\n",
        "\n",
        "\n",
        "def fe_test(X_test_, X_train_, y_train_):\n",
        "    X_test_ = X_test_.copy()\n",
        "    temp = pd.concat([X_train_, y_train_], axis=1)\n",
        "\n",
        "    X_test_[\"age>55\"] = (X_test_[\"age\"] > 55).astype(int)\n",
        "    for col in X_test_.columns:\n",
        "        if col == \"teacher_pred\":\n",
        "            continue\n",
        "        colname = f\"CAT_{col}\"\n",
        "        X_test_[colname] = X_test_[col].astype(str).astype(\"category\")\n",
        "\n",
        "    X_test_[\"chest_pain_type_bin\"] = (X_test_[\"chest_pain_type\"] >= 3).astype(int)\n",
        "    X_test_[\"st_depression_bin\"] = (X_test_[\"st_depression\"] >= 2).astype(int)\n",
        "    X_test_[\"number_of_vessels_fluro_bin\"] = (X_test_[\"number_of_vessels_fluro\"] >= 2).astype(int)\n",
        "    X_test_[\"hr_age\"] = X_test_[\"max_hr\"] * X_test_[\"age\"]\n",
        "    X_test_[\"bp_age\"] = X_test_[\"bp\"] * X_test_[\"age\"]\n",
        "    X_test_[\"st_slope\"] = X_test_[\"st_depression\"] * X_test_[\"slope_of_st\"]\n",
        "    X_test_[\"chol_exercise\"] = X_test_[\"cholesterol\"] * X_test_[\"exercise_angina\"]\n",
        "    pred_max = (220 - X_test_[\"age\"]).clip(lower=10)\n",
        "    X_test_[\"max_hr_pct_pred\"] = X_test_[\"max_hr\"] / pred_max\n",
        "    X_test_[\"risk_sum\"] = X_test_[\"number_of_vessels_fluro\"] + X_test_[\"thallium\"] + X_test_[\"exercise_angina\"]\n",
        "\n",
        "    global_mean = float(y_train_.mean())\n",
        "    m_smooth = 20\n",
        "    for bf in base_features:\n",
        "        stats = temp.groupby(bf)[ystr].agg(statmetrics)\n",
        "        for s in statmetrics:\n",
        "            X_test_[f\"target_{bf}_{s}\"] = X_test_[bf].map(stats[s])\n",
        "        smoothed = (stats[\"count\"] * stats[\"mean\"] + m_smooth * global_mean) / (stats[\"count\"] + m_smooth)\n",
        "        X_test_[f\"target_{bf}_smooth\"] = X_test_[bf].map(smoothed).fillna(global_mean)\n",
        "\n",
        "    for i, c1 in enumerate(cols2comb[:-1]):\n",
        "        for c2 in cols2comb[i + 1 :]:\n",
        "            m2 = max(X_train_[c2].max(), X_test_[c2].max()) + 1\n",
        "            cname = f\"{c1}_{c2}\"\n",
        "            X_test_[cname] = (\n",
        "                (X_test_[c1] + 1 + (X_test_[c2] + 1) / (m2 + 1)) * (m2 + 1)\n",
        "            ).astype(\"int16\")\n",
        "\n",
        "    if DROP_BP_MAX_HR:\n",
        "        X_test_ = X_test_.drop(columns=[\"bp\", \"max_hr\"], errors=\"ignore\")\n",
        "    return X_test_\n",
        "\n",
        "\n",
        "xgboost_params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"auc\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"max_depth\": 2,\n",
        "    \"subsample\": 0.9,\n",
        "    \"colsample_bytree\": 0.9,\n",
        "    \"n_estimators\": 2000,\n",
        "    \"min_child_weight\": 10,\n",
        "    \"gamma\": 1,\n",
        "    \"reg_lambda\": 0.01,\n",
        "    \"reg_alpha\": 1.5,\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"n_jobs\": -1,\n",
        "    \"random_state\": SEED,\n",
        "    \"early_stopping_rounds\": 100,\n",
        "    \"enable_categorical\": True,\n",
        "}\n",
        "\n",
        "catboost_params = {\n",
        "    \"loss_function\": \"Logloss\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"depth\": 2,\n",
        "    \"subsample\": 0.9,\n",
        "    \"iterations\": 2000,\n",
        "    \"min_data_in_leaf\": 1,\n",
        "    \"l2_leaf_reg\": 1.002,\n",
        "    \"thread_count\": -1,\n",
        "    \"random_seed\": SEED,\n",
        "    \"early_stopping_rounds\": 100,\n",
        "    \"bootstrap_type\": \"Bernoulli\",\n",
        "}\n",
        "\n",
        "lgbm_params = {\n",
        "    \"objective\": \"binary\",\n",
        "    \"metric\": \"auc\",\n",
        "    \"learning_rate\": 0.03,\n",
        "    \"n_estimators\": 1200,\n",
        "    \"num_leaves\": 20,\n",
        "    \"feature_fraction\": 0.9,\n",
        "    \"bagging_fraction\": 0.9,\n",
        "    \"bagging_freq\": 1,\n",
        "    \"min_data_in_leaf\": 30,\n",
        "    \"random_state\": SEED,\n",
        "    \"n_jobs\": -1,\n",
        "    \"verbose\": -1,\n",
        "}\n",
        "hgb_params = {\n",
        "    \"max_iter\": 2000,\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"max_depth\": 6,\n",
        "    \"early_stopping\": True,\n",
        "    \"n_iter_no_change\": 80,\n",
        "    \"validation_fraction\": 0.1,\n",
        "    \"random_state\": SEED,\n",
        "}\n",
        "models = {\n",
        "    XGBClassifier: xgboost_params,\n",
        "    CatBoostClassifier: catboost_params,\n",
        "    LGBMClassifier: lgbm_params,\n",
        "    HistGradientBoostingClassifier: hgb_params,\n",
        "}\n",
        "\n",
        "oof_list, test_proba_list = [], []\n",
        "for SEED in SEEDS:\n",
        "    teacher = cb.CatBoostClassifier(\n",
        "        iterations=400, depth=4, learning_rate=0.05, subsample=0.9,\n",
        "        colsample_bylevel=0.9, random_seed=SEED, verbose=0,\n",
        "    )\n",
        "    teacher.fit(orig_X[common_cols], orig_y)\n",
        "    X[\"teacher_pred\"] = teacher.predict_proba(X[common_cols])[:, 1]\n",
        "    X_test[\"teacher_pred\"] = teacher.predict_proba(X_test[common_cols])[:, 1]\n",
        "    adv_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
        "    oof_adv = np.zeros(len(adv_y), dtype=float)\n",
        "    for tr_adv, val_adv in adv_skf.split(adv_X, adv_y):\n",
        "        adv_clf = lgb.LGBMClassifier(\n",
        "            objective=\"binary\", metric=\"auc\", learning_rate=0.05, n_estimators=400,\n",
        "            num_leaves=31, feature_fraction=0.9, bagging_fraction=0.9, bagging_freq=1,\n",
        "            min_data_in_leaf=30, random_state=SEED, n_jobs=-1,\n",
        "        )\n",
        "        adv_clf.fit(adv_X.iloc[tr_adv], adv_y[tr_adv])\n",
        "        oof_adv[val_adv] = adv_clf.predict_proba(adv_X.iloc[val_adv])[:, 1]\n",
        "    p_test_train = oof_adv[: len(X)]\n",
        "    w_train = p_test_train / (1.0 - p_test_train + 1e-3)\n",
        "    w_train = w_train / w_train.mean()\n",
        "    xgboost_params = {**xgboost_params, \"random_state\": SEED}\n",
        "    catboost_params = {**catboost_params, \"random_seed\": SEED}\n",
        "    lgbm_params = {**lgbm_params, \"random_state\": SEED}\n",
        "    hgb_params_s = {**hgb_params, \"random_state\": SEED}\n",
        "    models_run = {\n",
        "        XGBClassifier: xgboost_params,\n",
        "        CatBoostClassifier: catboost_params,\n",
        "        LGBMClassifier: lgbm_params,\n",
        "        HistGradientBoostingClassifier: hgb_params_s,\n",
        "    }\n",
        "    kf = KFold(n_splits=NSPLITS, shuffle=True, random_state=SEED)\n",
        "    oof_train_model = {}\n",
        "    oof_test_model = {}\n",
        "    cv_auc_model = defaultdict(list)\n",
        "    for modelClass, param in models_run.items():\n",
        "        model_name = modelClass.__name__\n",
        "        oof_train = np.zeros(len(X))\n",
        "        oof_test = np.zeros(len(X_test))\n",
        "\n",
        "        for fold, (tr, val) in enumerate(kf.split(X)):\n",
        "            print(f\"FOLD {fold + 1}/{NSPLITS} - {model_name}\")\n",
        "\n",
        "            X_tr_raw, X_val_raw = X.iloc[tr], X.iloc[val]\n",
        "            y_tr, y_val = y.iloc[tr], y.iloc[val]\n",
        "\n",
        "            X_tr, X_val = fe_foldwise(X_tr_raw, X_val_raw, y_tr)\n",
        "\n",
        "            model = modelClass(**param)\n",
        "            if model_name == \"HistGradientBoostingClassifier\":\n",
        "                X_tr_fit = X_tr.select_dtypes(include=[np.number])\n",
        "                X_val_fit = X_val.select_dtypes(include=[np.number])\n",
        "                model.fit(X_tr_fit, y_tr, sample_weight=w_train[tr])\n",
        "                oof_train[val] = model.predict_proba(X_val_fit)[:, 1]\n",
        "                X_test_fe = fe_test(X_test, X_tr_raw, y_tr)\n",
        "                X_test_fit = X_test_fe.select_dtypes(include=[np.number])\n",
        "                oof_test += model.predict_proba(X_test_fit)[:, 1] / NSPLITS\n",
        "            else:\n",
        "                fit_kwargs = {\n",
        "                    \"X\": X_tr,\n",
        "                    \"y\": y_tr,\n",
        "                    \"eval_set\": [(X_val, y_val)],\n",
        "                    \"sample_weight\": w_train[tr],\n",
        "                }\n",
        "                if model_name != \"LGBMClassifier\":\n",
        "                    fit_kwargs[\"verbose\"] = 2000\n",
        "                if \"cat_features\" in inspect.signature(model.fit).parameters:\n",
        "                    cat_features = get_cat_feature_indices(X_tr)\n",
        "                    fit_kwargs[\"cat_features\"] = cat_features\n",
        "                model.fit(**fit_kwargs)\n",
        "                oof_train[val] = model.predict_proba(X_val)[:, 1]\n",
        "                X_test_fe = fe_test(X_test, X_tr_raw, y_tr)\n",
        "                oof_test += model.predict_proba(X_test_fe)[:, 1] / NSPLITS\n",
        "            cv_auc_model[model_name].append(roc_auc_score(y[val], oof_train[val]))\n",
        "\n",
        "        oof_train_model[model_name] = oof_train\n",
        "        oof_test_model[model_name] = oof_test\n",
        "\n",
        "    # Evaluation per model (inside seed loop)\n",
        "    for modelClass in models_run.keys():\n",
        "        model_name = modelClass.__name__\n",
        "        print(f\"\\n{model_name} OOF AUC: {roc_auc_score(y, oof_train_model[model_name]):.6f}\")\n",
        "        print(\n",
        "            f\"{model_name} CV AUC mean: {np.mean(cv_auc_model[model_name]):.6f}, std: +-{np.std(cv_auc_model[model_name]):.5f}\"\n",
        "        )\n",
        "\n",
        "    # Stack: LR meta for 3+ models or when USE_LR_STACK; else tuned 2-model weight blend\n",
        "    X_oof_tr = pd.DataFrame.from_dict(oof_train_model)\n",
        "    X_oof_test = pd.DataFrame.from_dict(oof_test_model)\n",
        "    cols = list(X_oof_tr.columns)\n",
        "    use_meta = USE_LR_STACK or len(cols) >= 3\n",
        "    if use_meta:\n",
        "        meta = LogisticRegression(max_iter=500, random_state=SEED)\n",
        "        meta.fit(X_oof_tr, y, sample_weight=w_train)\n",
        "        oof_tr_final = pd.Series(meta.predict_proba(X_oof_tr)[:, 1], index=X_oof_tr.index)\n",
        "        oof_test_final = pd.Series(meta.predict_proba(X_oof_test)[:, 1], index=X_oof_test.index)\n",
        "        stack_auc = roc_auc_score(y, oof_tr_final)\n",
        "        print(f\"\\nStack (LR meta, {len(cols)} models) OOF AUC: {stack_auc:.6f}\")\n",
        "        oof_list.append(oof_tr_final.values)\n",
        "        test_proba_list.append(oof_test_final.values)\n",
        "    else:\n",
        "        a, b = X_oof_tr[cols[0]], X_oof_tr[cols[1]]\n",
        "        best_w, best_auc = 0.5, 0.0\n",
        "        for w in np.linspace(0, 1, 21):\n",
        "            blend = w * a + (1 - w) * b\n",
        "            auc = roc_auc_score(y, blend)\n",
        "            if auc > best_auc:\n",
        "                best_auc, best_w = auc, w\n",
        "        oof_tr_final = best_w * X_oof_tr[cols[0]] + (1 - best_w) * X_oof_tr[cols[1]]\n",
        "        oof_test_final = best_w * X_oof_test[cols[0]] + (1 - best_w) * X_oof_test[cols[1]]\n",
        "        print(f\"\\nBlend weight {cols[0]}={best_w:.2f}, {cols[1]}={1-best_w:.2f} -> OOF AUC: {best_auc:.6f}\")\n",
        "        oof_list.append(oof_tr_final.values)\n",
        "        test_proba_list.append(oof_test_final.values)\n",
        "\n",
        "# Seed averaging (outside SEED loop)\n",
        "oof = np.mean(oof_list, axis=0)\n",
        "test_proba = np.mean(test_proba_list, axis=0)\n",
        "N_STACK_MODELS = len(oof_train_model)\n",
        "print(f\"Submission: {N_STACK_MODELS}-model stack, {len(SEEDS)}-seed avg. test_proba shape: {test_proba.shape}\")\n",
        "\n",
        "# Ensure id column exists for submission\n",
        "test[\"id\"] = test.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion matrix (OOF, threshold=0.5)\n",
            "Rows: true, Cols: predicted |  Absence   Presence\n",
            "[[314911  32635]\n",
            " [ 37226 245228]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "y_pred = (oof >= 0.5).astype(int)\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "print(\"Confusion matrix (OOF, threshold=0.5)\")\n",
        "print(\"Rows: true, Cols: predicted |  Absence   Presence\")\n",
        "print(cm)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(cm, cmap=\"Blues\")\n",
        "plt.colorbar()\n",
        "plt.xticks([0, 1], [\"Absence\", \"Presence\"])\n",
        "plt.yticks([0, 1], [\"Absence\", \"Presence\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
        "plt.title(\"Confusion matrix (OOF)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submission saved (4-model blend): submission.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Heart Disease</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>630000</th>\n",
              "      <td>630000</td>\n",
              "      <td>0.948956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630001</th>\n",
              "      <td>630001</td>\n",
              "      <td>0.039575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630002</th>\n",
              "      <td>630002</td>\n",
              "      <td>0.959675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630003</th>\n",
              "      <td>630003</td>\n",
              "      <td>0.038572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630004</th>\n",
              "      <td>630004</td>\n",
              "      <td>0.137692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630005</th>\n",
              "      <td>630005</td>\n",
              "      <td>0.958631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630006</th>\n",
              "      <td>630006</td>\n",
              "      <td>0.037886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630007</th>\n",
              "      <td>630007</td>\n",
              "      <td>0.573713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630008</th>\n",
              "      <td>630008</td>\n",
              "      <td>0.960338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630009</th>\n",
              "      <td>630009</td>\n",
              "      <td>0.040666</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  Heart Disease\n",
              "id                           \n",
              "630000  630000       0.948956\n",
              "630001  630001       0.039575\n",
              "630002  630002       0.959675\n",
              "630003  630003       0.038572\n",
              "630004  630004       0.137692\n",
              "630005  630005       0.958631\n",
              "630006  630006       0.037886\n",
              "630007  630007       0.573713\n",
              "630008  630008       0.960338\n",
              "630009  630009       0.040666"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub = pd.DataFrame({\"id\": test[\"id\"], \"Heart Disease\": test_proba})\n",
        "sub.to_csv(OUTPUT_DIR / \"submission.csv\", index=False)\n",
        "print(f\"Submission saved ({N_STACK_MODELS}-model blend): {OUTPUT_DIR / 'submission.csv'}\")\n",
        "sub.head(10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
